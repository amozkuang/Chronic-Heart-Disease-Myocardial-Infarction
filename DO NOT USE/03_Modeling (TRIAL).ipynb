{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "\n",
    "# DSI-SG-42 Project 3: Web APIs & NLP\n",
    "### Reddit Scams: Are We Vulnerable?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Import scraped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>bmi</th>\n",
       "      <th>yrssmok</th>\n",
       "      <th>packday</th>\n",
       "      <th>yrsquit</th>\n",
       "      <th>sleep_hours</th>\n",
       "      <th>health_status</th>\n",
       "      <th>phys_health_not_good</th>\n",
       "      <th>...</th>\n",
       "      <th>chd_mi</th>\n",
       "      <th>asthma_status</th>\n",
       "      <th>race_ethnicity</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>income</th>\n",
       "      <th>smoker_status</th>\n",
       "      <th>e_cig_smoker</th>\n",
       "      <th>binge_drinker</th>\n",
       "      <th>heavy_drinker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Never</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>Grad College or Tech Sch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Never smoked</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>68.04</td>\n",
       "      <td>26.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Never</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>Grad High Sch</td>\n",
       "      <td>$25k-$35k</td>\n",
       "      <td>Never smoked</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.57</td>\n",
       "      <td>63.50</td>\n",
       "      <td>25.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>1-13 days</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Never</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>Grad College or Tech Sch</td>\n",
       "      <td>$100k-$200k</td>\n",
       "      <td>Never smoked</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>63.50</td>\n",
       "      <td>23.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Current</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>Grad High Sch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Current smoker - some days</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.57</td>\n",
       "      <td>53.98</td>\n",
       "      <td>21.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Fair</td>\n",
       "      <td>1-13 days</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Never</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>Attended College or Tech Sch</td>\n",
       "      <td>$25k-$35k</td>\n",
       "      <td>Never smoked</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  height  weight    bmi  yrssmok  packday  yrsquit  sleep_hours  \\\n",
       "0  80.0     NaN     NaN    NaN      0.0      0.0      0.0          8.0   \n",
       "1  80.0    1.60   68.04  26.58      0.0      0.0      0.0          6.0   \n",
       "2  56.0    1.57   63.50  25.76      0.0      0.0      0.0          5.0   \n",
       "3  73.0    1.65   63.50  23.32      0.0      0.0      0.0          7.0   \n",
       "4  43.0    1.57   53.98  21.90      0.0      0.0      0.0          9.0   \n",
       "\n",
       "  health_status phys_health_not_good  ... chd_mi asthma_status race_ethnicity  \\\n",
       "0     Very Good               0 days  ...     No         Never          White   \n",
       "1     Excellent               0 days  ...     No         Never          White   \n",
       "2     Very Good            1-13 days  ...     No         Never          White   \n",
       "3     Excellent               0 days  ...     No       Current          White   \n",
       "4          Fair            1-13 days  ...     No         Never          White   \n",
       "\n",
       "      sex                     education       income  \\\n",
       "0  Female      Grad College or Tech Sch          NaN   \n",
       "1  Female                 Grad High Sch    $25k-$35k   \n",
       "2  Female      Grad College or Tech Sch  $100k-$200k   \n",
       "3  Female                 Grad High Sch          NaN   \n",
       "4  Female  Attended College or Tech Sch    $25k-$35k   \n",
       "\n",
       "                smoker_status e_cig_smoker binge_drinker heavy_drinker  \n",
       "0                Never smoked           No            No            No  \n",
       "1                Never smoked           No            No            No  \n",
       "2                Never smoked           No            No            No  \n",
       "3  Current smoker - some days           No            No            No  \n",
       "4                Never smoked           No            No            No  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_data_new.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 First look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 445132 entries, 0 to 445131\n",
      "Data columns (total 29 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   age                        445132 non-null  float64\n",
      " 1   height                     416480 non-null  float64\n",
      " 2   weight                     403054 non-null  float64\n",
      " 3   bmi                        398564 non-null  float64\n",
      " 4   yrssmok                    412104 non-null  float64\n",
      " 5   packday                    412040 non-null  float64\n",
      " 6   yrsquit                    409594 non-null  float64\n",
      " 7   sleep_hours                439679 non-null  float64\n",
      " 8   health_status              443934 non-null  object \n",
      " 9   phys_health_not_good       434205 non-null  object \n",
      " 10  mental_health_not_good     436065 non-null  object \n",
      " 11  last_routine_checkup       439333 non-null  object \n",
      " 12  visit_dentist_past_year    438111 non-null  object \n",
      " 13  health_insurance           427247 non-null  object \n",
      " 14  phy_exercise_past_30_days  444039 non-null  object \n",
      " 15  stroke                     443575 non-null  object \n",
      " 16  cancer                     442711 non-null  object \n",
      " 17  kidney_disease             443206 non-null  object \n",
      " 18  colon_sigmoidoscopy        422950 non-null  object \n",
      " 19  chd_mi                     440111 non-null  object \n",
      " 20  asthma_status              441272 non-null  object \n",
      " 21  race_ethnicity             431075 non-null  object \n",
      " 22  sex                        445132 non-null  object \n",
      " 23  education                  442749 non-null  object \n",
      " 24  income                     349085 non-null  object \n",
      " 25  smoker_status              409670 non-null  object \n",
      " 26  e_cig_smoker               409472 non-null  object \n",
      " 27  binge_drinker              394030 non-null  object \n",
      " 28  heavy_drinker              395427 non-null  object \n",
      "dtypes: float64(8), object(21)\n",
      "memory usage: 98.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                              0\n",
       "height                       28652\n",
       "weight                       42078\n",
       "bmi                          46568\n",
       "yrssmok                      33028\n",
       "packday                      33092\n",
       "yrsquit                      35538\n",
       "sleep_hours                   5453\n",
       "health_status                 1198\n",
       "phys_health_not_good         10927\n",
       "mental_health_not_good        9067\n",
       "last_routine_checkup          5799\n",
       "visit_dentist_past_year       7021\n",
       "health_insurance             17885\n",
       "phy_exercise_past_30_days     1093\n",
       "stroke                        1557\n",
       "cancer                        2421\n",
       "kidney_disease                1926\n",
       "colon_sigmoidoscopy          22182\n",
       "chd_mi                        5021\n",
       "asthma_status                 3860\n",
       "race_ethnicity               14057\n",
       "sex                              0\n",
       "education                     2383\n",
       "income                       96047\n",
       "smoker_status                35462\n",
       "e_cig_smoker                 35660\n",
       "binge_drinker                51102\n",
       "heavy_drinker                49705\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Initial Modelling (Baseline Scores only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.0 LazyPredict to get Baseline scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lazypredict in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (0.2.12)\n",
      "Requirement already satisfied: click in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from lazypredict) (8.1.7)\n",
      "Requirement already satisfied: scikit-learn in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from lazypredict) (1.4.1.post1)\n",
      "Requirement already satisfied: pandas in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from lazypredict) (2.2.1)\n",
      "Requirement already satisfied: tqdm in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from lazypredict) (4.66.2)\n",
      "Requirement already satisfied: joblib in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from lazypredict) (1.3.2)\n",
      "Requirement already satisfied: lightgbm in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from lazypredict) (4.3.0)\n",
      "Requirement already satisfied: xgboost in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from lazypredict) (2.0.3)\n",
      "Requirement already satisfied: numpy in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from lightgbm->lazypredict) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from lightgbm->lazypredict) (1.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from pandas->lazypredict) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from pandas->lazypredict) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from pandas->lazypredict) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from scikit-learn->lazypredict) (3.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/moshimozzie/GA/sandbox/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->lazypredict) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lazypredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.8.0.1 Classifier Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlazypredict\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyClassifier\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'df' is your DataFrame and 'target' is the name of the target column\u001b[39;00m\n",
      "File \u001b[0;32m~/GA/sandbox/.venv/lib/python3.11/site-packages/lazypredict/Supervised.py:98\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# CLASSIFIERS.append(('CatBoostClassifier',catboost.CatBoostClassifier))\u001b[39;00m\n\u001b[1;32m     91\u001b[0m numeric_transformer \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m     92\u001b[0m     steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler())]\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m categorical_transformer_low \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m     96\u001b[0m     steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     97\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m---> 98\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m),\n\u001b[1;32m     99\u001b[0m     ]\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m categorical_transformer_high \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m    103\u001b[0m     steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    104\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     ]\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Helper function\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'target' is the name of the target column\n",
    "\n",
    "X = df.drop('chd_mi', axis=1)\n",
    "y = df['chd_mi']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit LazyClassifier\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# models is a DataFrame that contains the performance metrics for each model\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.8.0.2 Regressor Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [02:52<00:00,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Adjusted R-Squared, R-Squared, RMSE, Time Taken]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'target' is the name of the target column for regression\n",
    "\n",
    "X = df.drop('chd_mi', axis=1)\n",
    "y = df['chd_mi']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit LazyRegressor\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# models is a DataFrame that contains the performance metrics for each model\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.1 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that there were many comments/ replies that were auto-generated by bot. These do not serve much meaning to our analysis, hence we will remove these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/moshimozzie/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /Users/moshimozzie/anaconda3/lib/python3.11/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/moshimozzie/anaconda3/lib/python3.11/site-packages (from xgboost) (1.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the target variable 'cvdcrhd4' is nan\n",
    "df = df.dropna(subset=['chd_mi'])\n",
    "# If you decide to remap, here is how you could do it for binary classification\n",
    "mapping = {'No': 0, 'Yes': 1}\n",
    "df['chd_mi'] = df['chd_mi'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column name in the dropna() method should match the column you're intending to use\n",
    "df = df.dropna(subset=['chd_mi'])  # Make sure 'chd_mi' is the correct column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('chd_mi', axis=1)\n",
    "y = df['chd_mi']\n",
    "\n",
    "# Ensure y is of integer type\n",
    "y = y.astype(int)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object dtypes to category\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'object':\n",
    "        X_train[col] = X_train[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 91.19%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the XGBoost classifier with enable_categorical=True\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', enable_categorical=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 91.92%\n",
      "Test Accuracy: 91.19%\n",
      "CV Mean Score: 91.19%\n",
      "CV Scores per Fold: ['91.18%', '91.19%', '91.22%', '91.19%', '91.18%']\n"
     ]
    }
   ],
   "source": [
    "# Train score\n",
    "train_pred = model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Test score\n",
    "test_pred = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Preprocessing for CV score\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = X[col].astype('category')\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# Print the average of the cross-validation scores\n",
    "print(f\"CV Mean Score: {cv_scores.mean() * 100:.2f}%\")\n",
    "\n",
    "# Optionally, you can also look at the individual folds' scores\n",
    "print(f\"CV Scores per Fold: {[f'{score * 100:.2f}%' for score in cv_scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/moshimozzie/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/moshimozzie/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/moshimozzie/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/moshimozzie/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/moshimozzie/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('chd_mi', axis=1)\n",
    "y = df['chd_mi']\n",
    "\n",
    "# Impute missing values in features\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame if needed\n",
    "X_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "X_encoded = pd.get_dummies(X_imputed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.99%\n",
      "Test Accuracy: 91.17%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, bootstrap=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "# Predictions for evaluation\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Train and Test accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Preprocessing for CV score\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = X[col].astype('category')\n",
    "\n",
    "# Perform 5-fold cross-validation to evaluate the model\n",
    "cv_scores = cross_val_score(model, X_encoded, y, cv=5)\n",
    "\n",
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "cv_mean = cv_scores.mean()\n",
    "cv_std = cv_scores.std()\n",
    "print(f\"CV Mean Score: {cv_mean * 100:.2f}%\")\n",
    "print(f\"CV Standard Deviation: {cv_std * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Correctly create a preprocessing pipeline for numerical and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Add OneHotEncoder for categorical data\n",
    "])\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 97.96%\n",
      "Test Accuracy: 86.00%\n",
      "CV Mean Score: 85.92%\n",
      "CV Standard Deviation: 0.17%\n"
     ]
    }
   ],
   "source": [
    "# Proceed with training and evaluating the Decision Tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Preprocessing for CV score\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = X[col].astype('category')\n",
    "        \n",
    "cv_scores = cross_val_score(model, X_preprocessed, y, cv=5)\n",
    "cv_mean = cv_scores.mean()\n",
    "cv_std = cv_scores.std()\n",
    "print(f\"CV Mean Score: {cv_mean * 100:.2f}%\")\n",
    "print(f\"CV Standard Deviation: {cv_std * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Modelling (with parameters)\n",
    "\n",
    "We will only run the models on df2. The reason being - when we dropped values, there were 0 rows for df1 - hence there is no point running model on an empty dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Pipeline\n",
    "logistic_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# SVM Pipeline\n",
    "svm_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# XGBoost Pipeline\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    # No scaler needed for tree-based models\n",
    "    ('model', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n",
    "])\n",
    "\n",
    "# Random Forest Pipeline\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    # No scaler needed for tree-based models\n",
    "    ('model', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__solver': ['saga'],\n",
    "    'model__max_iter': [100, 1000, 5000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_param_grid = {\n",
    "    'model__C': [1, 10],  # Reduced range of C values\n",
    "    'model__gamma': ['scale', 0.01],  # Limited gamma to 'scale' and a representative value\n",
    "    'model__kernel': ['rbf'],  # Focus on the RBF kernel, often the best choice for SVM\n",
    "    # Removed 'model__degree': Typically, 'poly' kernel requires more processing time\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\n",
    "    'model__max_depth': [5, 7],  # Focused on mid-range depths\n",
    "    'model__n_estimators': [100, 200],  # Reduced the upper range\n",
    "    'model__learning_rate': [0.1],  # Chose a commonly effective rate\n",
    "    'model__subsample': [0.7, 1.0],  # Limited to higher subsampling for variance reduction\n",
    "    'model__colsample_bytree': [0.7],  # Chose a moderate value for feature sampling\n",
    "    'model__gamma': [0, 0.1]  # Simplified to two options to evaluate regularization benefit\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # Reduced number of trees options\n",
    "    'model__max_depth': [10, 20],  # Focus on moderate to high depth to control complexity\n",
    "    'model__min_samples_split': [2, 5],  # Simplified range for minimum number of samples required to split\n",
    "    'model__min_samples_leaf': [1, 2],  # Reduced range for the minimum number of samples required at a leaf node\n",
    "    'model__max_features': ['auto'],  # Use the default option for the number of features to consider when looking for the best split\n",
    "    'model__bootstrap': [True]  # Keep bootstrapping enabled for better generalization\n",
    "    # Removed 'model__criterion': Simplification, sticking with the default 'gini' criterion\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'model__C': 0.01, 'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'saga'}\n",
      "Best CV score for Logistic Regression: 0.910648858406281\n",
      "Train accuracy for Logistic Regression: 0.9106458502541889\n",
      "Test accuracy for Logistic Regression: 0.9107546806564951\n",
      "Mean CV score for the best Logistic Regression model: 0.9106 ± 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression GridSearchCV\n",
    "logistic_grid_search = GridSearchCV(logistic_pipeline, logistic_param_grid, cv=5, scoring='accuracy')\n",
    "logistic_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and CV score\n",
    "print(\"Best parameters for Logistic Regression:\", logistic_grid_search.best_params_)\n",
    "print(\"Best CV score for Logistic Regression:\", logistic_grid_search.best_score_)\n",
    "\n",
    "# Evaluate on training data using the best estimator found by GridSearchCV\n",
    "y_train_pred = logistic_grid_search.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train accuracy for Logistic Regression:\", train_accuracy)\n",
    "\n",
    "# Evaluate on test data using the best estimator found by GridSearchCV\n",
    "y_test_pred = logistic_grid_search.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test accuracy for Logistic Regression:\", test_accuracy)\n",
    "\n",
    "# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n",
    "# Here's how to get the mean CV score for the best estimator across folds\n",
    "best_index = logistic_grid_search.best_index_\n",
    "mean_cv_score = logistic_grid_search.cv_results_['mean_test_score'][best_index]\n",
    "std_cv_score = logistic_grid_search.cv_results_['std_test_score'][best_index]\n",
    "print(f\"Mean CV score for the best Logistic Regression model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# SVM GridSearchCV\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m svm_grid_search \u001b[38;5;241m=\u001b[39m \u001b[43mGridSearchCV\u001b[49m(svm_pipeline, svm_param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m svm_grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Best parameters and score\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# SVM GridSearchCV\n",
    "svm_grid_search = GridSearchCV(svm_pipeline, svm_param_grid, cv=5, scoring='accuracy')\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters for SVM:\", svm_grid_search.best_params_)\n",
    "print(\"Best score for SVM:\", svm_grid_search.best_score_)\n",
    "\n",
    "# Evaluate on training data using the best estimator found by GridSearchCV\n",
    "y_train_pred = svm_grid_search.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train accuracy for SVM\", train_accuracy)\n",
    "\n",
    "# Evaluate on test data using the best estimator found by GridSearchCV\n",
    "y_test_pred = svm_grid_search.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test accuracy for SVM:\", test_accuracy)\n",
    "\n",
    "# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n",
    "# Here's how to get the mean CV score for the best estimator across folds\n",
    "best_index = svm_grid_search.best_index_\n",
    "mean_cv_score = svm_grid_search.cv_results_['mean_test_score'][best_index]\n",
    "std_cv_score = svm_grid_search.cv_results_['std_test_score'][best_index]\n",
    "print(f\"Mean CV score for the best SVM model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost GridSearchCV\n",
    "xgb_grid_search = GridSearchCV(xgb_pipeline, xgb_param_grid, cv=5, scoring='accuracy')\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters for XGBoost:\", xgb_grid_search.best_params_)\n",
    "print(\"Best score for XGBoost:\", xgb_grid_search.best_score_)\n",
    "\n",
    "# Evaluate on training data using the best estimator found by GridSearchCV\n",
    "y_train_pred = xgb_grid_search.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train accuracy for XGBoost:\", train_accuracy)\n",
    "\n",
    "# Evaluate on test data using the best estimator found by GridSearchCV\n",
    "y_test_pred = xgb_grid_search.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test accuracy for XGBoost:\", test_accuracy)\n",
    "\n",
    "# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n",
    "# Here's how to get the mean CV score for the best estimator across folds\n",
    "best_index = xgb_grid_search.best_index_\n",
    "mean_cv_score = xgb_grid_search.cv_results_['mean_test_score'][best_index]\n",
    "std_cv_score = xgb_grid_search.cv_results_['std_test_score'][best_index]\n",
    "print(f\"Mean CV score for the best XGBoost model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest GridSearchCV\n",
    "rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='accuracy')\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters for Random Forest:\", rf_grid_search.best_params_)\n",
    "print(\"Best score for Random Forest:\", rf_grid_search.best_score_)\n",
    "\n",
    "# Evaluate on training data using the best estimator found by GridSearchCV\n",
    "y_train_pred = rf_grid_search.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train accuracy for Random Forest:\", train_accuracy)\n",
    "\n",
    "# Evaluate on test data using the best estimator found by GridSearchCV\n",
    "y_test_pred = rf_grid_search.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test accuracy for Random Forest:\", test_accuracy)\n",
    "\n",
    "# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n",
    "# Here's how to get the mean CV score for the best estimator across folds\n",
    "best_index = rf_grid_search.best_index_\n",
    "mean_cv_score = rf_grid_search.cv_results_['mean_test_score'][best_index]\n",
    "std_cv_score = rf_grid_search.cv_results_['std_test_score'][best_index]\n",
    "print(f\"Mean CV score for the best Random Forest model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 Export data to CSV to proceed with EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.to_csv('../data/cleaned_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
