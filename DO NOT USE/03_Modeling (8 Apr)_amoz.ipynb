{"cells":[{"cell_type":"markdown","metadata":{"id":"cpi2tMWgYBys"},"source":["<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n","\n","\n","# DSI-SG-42 Project 3: Web APIs & NLP\n","### Reddit Scams: Are We Vulnerable?\n","---"]},{"cell_type":"markdown","metadata":{"id":"Y0xJqFM4YByx"},"source":["## 2. Data Cleaning"]},{"cell_type":"markdown","metadata":{"id":"L3-mvMN_YByx"},"source":["### 2.1 Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHJQ_1UzYByy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import xgboost as xgb\n","import time\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"xrHxP_LmYBy0"},"source":["### 2.2 Import scraped dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"id":"vwebSFSrYBy1","executionInfo":{"status":"ok","timestamp":1712590569347,"user_tz":-480,"elapsed":503,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"335ac440-0e53-479c-aba6-bf905d16cb9f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    age  height  weight     bmi  yrssmok  packday  yrsquit  sleep_hours  \\\n","0  80.0   1.698  82.372  21.166      0.0      0.0      0.0          8.0   \n","1  80.0   1.600  68.040  26.580      0.0      0.0      0.0          6.0   \n","2  56.0   1.570  63.500  25.760      0.0      0.0      0.0          5.0   \n","3  73.0   1.650  63.500  23.320     56.0      0.1      8.2          7.0   \n","4  43.0   1.570  53.980  21.900      0.0      0.0      0.0          9.0   \n","\n","   health_status  phys_health_not_good  ...  colon_sigmoidoscopy  \\\n","0            2.0                   1.0  ...                  1.0   \n","1            1.0                   1.0  ...                  1.0   \n","2            2.0                   2.0  ...                  2.0   \n","3            1.0                   1.0  ...                  1.0   \n","4            4.0                   2.0  ...                999.0   \n","\n","   asthma_status  race_ethnicity  sex  education  income  smoker_status  \\\n","0            3.0             1.0  2.0        4.0     7.0            4.0   \n","1            3.0             1.0  2.0        2.0     5.0            4.0   \n","2            3.0             1.0  2.0        4.0    10.0            4.0   \n","3            1.0             1.0  2.0        2.0     7.0            2.0   \n","4            3.0             1.0  2.0        3.0     5.0            4.0   \n","\n","   e_cig_smoker  binge_drinker  heavy_drinker  \n","0           1.0            1.0            1.0  \n","1           1.0            1.0            1.0  \n","2           1.0            1.0            1.0  \n","3           1.0            1.0            1.0  \n","4           1.0            1.0            1.0  \n","\n","[5 rows x 28 columns]"],"text/html":["\n","  <div id=\"df-a23c229c-b330-46ef-ac85-262925149d3e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>bmi</th>\n","      <th>yrssmok</th>\n","      <th>packday</th>\n","      <th>yrsquit</th>\n","      <th>sleep_hours</th>\n","      <th>health_status</th>\n","      <th>phys_health_not_good</th>\n","      <th>...</th>\n","      <th>colon_sigmoidoscopy</th>\n","      <th>asthma_status</th>\n","      <th>race_ethnicity</th>\n","      <th>sex</th>\n","      <th>education</th>\n","      <th>income</th>\n","      <th>smoker_status</th>\n","      <th>e_cig_smoker</th>\n","      <th>binge_drinker</th>\n","      <th>heavy_drinker</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>80.0</td>\n","      <td>1.698</td>\n","      <td>82.372</td>\n","      <td>21.166</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>7.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>80.0</td>\n","      <td>1.600</td>\n","      <td>68.040</td>\n","      <td>26.580</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>56.0</td>\n","      <td>1.570</td>\n","      <td>63.500</td>\n","      <td>25.760</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>10.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>73.0</td>\n","      <td>1.650</td>\n","      <td>63.500</td>\n","      <td>23.320</td>\n","      <td>56.0</td>\n","      <td>0.1</td>\n","      <td>8.2</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>43.0</td>\n","      <td>1.570</td>\n","      <td>53.980</td>\n","      <td>21.900</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>999.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 28 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a23c229c-b330-46ef-ac85-262925149d3e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a23c229c-b330-46ef-ac85-262925149d3e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a23c229c-b330-46ef-ac85-262925149d3e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0e807f26-02ae-447b-a3ea-1ebf8f16dcbe\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0e807f26-02ae-447b-a3ea-1ebf8f16dcbe')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0e807f26-02ae-447b-a3ea-1ebf8f16dcbe button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":4}],"source":["df = pd.read_csv('final_dataset.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"_sbP7ItQYBy2"},"source":["### 2.3 First look at data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xpy8wg34YBy3","executionInfo":{"status":"ok","timestamp":1712590576461,"user_tz":-480,"elapsed":403,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"abd5d247-e012-46cc-d3fa-c08b06a16883"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 60821 entries, 0 to 60820\n","Data columns (total 28 columns):\n"," #   Column                     Non-Null Count  Dtype  \n","---  ------                     --------------  -----  \n"," 0   age                        60821 non-null  float64\n"," 1   height                     60821 non-null  float64\n"," 2   weight                     60821 non-null  float64\n"," 3   bmi                        60821 non-null  float64\n"," 4   yrssmok                    60821 non-null  float64\n"," 5   packday                    60821 non-null  float64\n"," 6   yrsquit                    60821 non-null  float64\n"," 7   sleep_hours                60821 non-null  float64\n"," 8   health_status              60821 non-null  float64\n"," 9   phys_health_not_good       60821 non-null  float64\n"," 10  mental_health_not_good     60821 non-null  float64\n"," 11  last_routine_checkup       60821 non-null  float64\n"," 12  visit_dentist_past_year    60821 non-null  float64\n"," 13  health_insurance           60821 non-null  float64\n"," 14  phy_exercise_past_30_days  60821 non-null  float64\n"," 15  stroke                     60821 non-null  float64\n"," 16  cancer                     60821 non-null  float64\n"," 17  kidney_disease             60821 non-null  float64\n"," 18  colon_sigmoidoscopy        60821 non-null  float64\n"," 19  asthma_status              60821 non-null  float64\n"," 20  race_ethnicity             60821 non-null  float64\n"," 21  sex                        60821 non-null  float64\n"," 22  education                  60821 non-null  float64\n"," 23  income                     60821 non-null  float64\n"," 24  smoker_status              60820 non-null  float64\n"," 25  e_cig_smoker               60820 non-null  float64\n"," 26  binge_drinker              60820 non-null  float64\n"," 27  heavy_drinker              60820 non-null  float64\n","dtypes: float64(28)\n","memory usage: 13.0 MB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"IGBo8LFHYBy4"},"source":["### 2.7 Check for null values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9eHUWXpYBy4","executionInfo":{"status":"ok","timestamp":1712590618217,"user_tz":-480,"elapsed":416,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"ba813aee-6d60-463c-eb55-b2be8bc6950c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["age                          0\n","height                       0\n","weight                       0\n","bmi                          0\n","yrssmok                      0\n","packday                      0\n","yrsquit                      0\n","sleep_hours                  0\n","health_status                0\n","phys_health_not_good         0\n","mental_health_not_good       0\n","last_routine_checkup         0\n","visit_dentist_past_year      0\n","health_insurance             0\n","phy_exercise_past_30_days    0\n","stroke                       0\n","cancer                       0\n","kidney_disease               0\n","colon_sigmoidoscopy          0\n","asthma_status                0\n","race_ethnicity               0\n","sex                          0\n","education                    0\n","income                       0\n","smoker_status                1\n","e_cig_smoker                 1\n","binge_drinker                1\n","heavy_drinker                1\n","dtype: int64"]},"metadata":{},"execution_count":6}],"source":["df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"DfoeTKBbYBy5"},"source":["### 2.8 Initial Modelling (Baseline Scores only)"]},{"cell_type":"code","source":["# Check if there are any missing values in the dataset\n","if df.isnull().sum().any():\n","    print(\"There are missing values in the dataset.\")\n","else:\n","    print(\"There are no missing values in the dataset.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-7RLnmdWK-r","executionInfo":{"status":"ok","timestamp":1712590623963,"user_tz":-480,"elapsed":519,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"4ed902cf-31e7-41df-8683-91194205739c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are missing values in the dataset.\n"]}]},{"cell_type":"markdown","source":["#### 2.8.1 Logistic Regression"],"metadata":{"id":"Y_Td2ydgLpzk"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","import pandas as pd"],"metadata":{"id":"XtBqlt6sL4ci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separate features and target\n","X = df.drop('chd_mi', axis=1)\n","y = df['chd_mi'].astype(int)\n","\n","# Map the values of y from [1, 2] to [0, 1]\n","y_mapped = y.map({1: 0, 2: 1})\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y_mapped, test_size=0.2, stratify=y_mapped, random_state=42)"],"metadata":{"id":"g4JfAtjjL4ON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Identify categorical columns\n","categorical_cols = X_train.select_dtypes(include=['object', 'bool']).columns.tolist()\n","\n","# Identify numerical columns\n","numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()"],"metadata":{"id":"vo6VbcQLL4B-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the preprocessing pipelines for both numerical and categorical data\n","numerical_transformer = Pipeline(steps=[\n","    ('scaler', StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])"],"metadata":{"id":"tU0STPpfNNSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine preprocessing steps\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numerical_transformer, numerical_cols),\n","        ('cat', categorical_transformer, categorical_cols)\n","    ])\n","\n","# Create a preprocessing and modeling pipeline\n","logreg_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n","                                  ('model', LogisticRegression(max_iter=1000, random_state=42))])\n","\n","# Train the model\n","logreg_pipeline.fit(X_train, y_train)"],"metadata":{"id":"bvtbdqR2YJup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions\n","y_train_pred = logreg_pipeline.predict(X_train)\n","y_test_pred = logreg_pipeline.predict(X_test)\n","\n","# Calculate and print the accuracies\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","\n","# Perform 5-fold cross-validation and print the average accuracy\n","cv_scores = cross_val_score(logreg_pipeline, X, y_mapped, cv=5, scoring='accuracy')\n","print(f\"CV Mean Score: {cv_scores.mean() * 100:.2f}%\")\n","print(f\"CV Scores per Fold: {[f'{score * 100:.2f}%' for score in cv_scores]}\")"],"metadata":{"id":"HUQFX35IYKGF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jOay_pD4YBy7"},"source":["#### 2.8.1 XGBoost"]},{"cell_type":"markdown","metadata":{"id":"cPsT_Za_YBy7"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CST8I2BiYBy7","executionInfo":{"status":"ok","timestamp":1712565316005,"user_tz":-480,"elapsed":6027,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"977d1747-90fd-45ce-cc9d-3dbe32eedb16"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n"]}],"source":["pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R46T54cqYBy8"},"outputs":[],"source":["# The column name in the dropna() method should match the column you're intending to use\n","df = df.dropna(subset=['chd_mi'])  # Make sure 'chd_mi' is the correct column name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_r6rrtLgYBy8"},"outputs":[],"source":["# Separate features and target\n","X = df.drop('chd_mi', axis=1)\n","y = df['chd_mi'].astype(int)\n","\n","# Map the values of y from [1, 2] to [0, 1]\n","y_mapped = y.map({1: 0, 2: 1})\n","\n","# Verify the consistency in the number of samples between X and y_mapped\n","assert len(X) == len(y_mapped), \"The feature set X and target variable y_mapped have inconsistent lengths.\"\n","\n","# Now, you can safely perform the train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y_mapped, test_size=0.2, stratify=y_mapped, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2FayISJYBy9","executionInfo":{"status":"ok","timestamp":1712565318541,"user_tz":-480,"elapsed":1903,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"9db40872-f9bd-44fa-a2b4-ee9a9d92891c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 91.09%\n"]}],"source":["# Initialize the XGBoost classifier with enable_categorical=True\n","model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', enable_categorical=True)\n","\n","# Train the model using the correct y_train variable\n","model.fit(X_train, y_train)  # Use y_train directly after ensuring it's correctly mapped and split\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model on the test set\n","accuracy = accuracy_score(y_test, y_pred)  # Use y_test which corresponds to the split and mapped target variable\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2r2HPHOYBy9","executionInfo":{"status":"ok","timestamp":1712565328137,"user_tz":-480,"elapsed":9599,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"3294d053-50cf-44d6-917b-a1bc1cfb5252"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Accuracy: 91.61%\n","Test Accuracy: 91.09%\n","CV Mean Score: 91.08%\n","CV Scores per Fold: ['91.05%', '91.09%', '91.05%', '91.08%', '91.11%']\n"]}],"source":["from sklearn.model_selection import cross_val_score\n","\n","# Make predictions on training and test sets\n","train_pred = model.predict(X_train)\n","test_pred = model.predict(X_test)\n","\n","# Calculate and print training and test accuracies\n","# Here, make sure to use 'y_train' and 'y_test' which are the variables you should have defined after the train-test split and mapping\n","train_accuracy = accuracy_score(y_train, train_pred)\n","test_accuracy = accuracy_score(y_test, test_pred)\n","print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","\n","# Ensure X is suitable for cross-validation by converting object types to 'category' if needed\n","# This step might not be necessary for models like XGBoost when using enable_categorical=True\n","# but is kept for demonstration or if you plan to use models that do not natively support categorical features\n","X_for_cv = X.copy()\n","for col in X_for_cv.columns:\n","    if X_for_cv[col].dtype == 'object':\n","        X_for_cv[col] = X_for_cv[col].astype('category')\n","\n","# Perform 5-fold cross-validation using the mapped y\n","cv_scores = cross_val_score(model, X_for_cv, y_mapped, cv=5, scoring='accuracy')\n","\n","# Print the average of the cross-validation scores and the scores for each fold\n","print(f\"CV Mean Score: {cv_scores.mean() * 100:.2f}%\")\n","print(f\"CV Scores per Fold: {[f'{score * 100:.2f}%' for score in cv_scores]}\")"]},{"cell_type":"markdown","metadata":{"id":"B_wghPpPYBy-"},"source":["#### 2.8.2 Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-lh7jjDYBy-","executionInfo":{"status":"ok","timestamp":1712565333888,"user_tz":-480,"elapsed":5767,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"af16baad-4f49-49a6-f97e-fd8dc8e05afa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"]}],"source":["pip install scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsKVgm7LYBy-"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Separate features and target\n","X = df.drop('chd_mi', axis=1)\n","y = df['chd_mi']\n","\n","# Transform 'X'\n","X = X.fit_transform(X)\n","\n","X_encoded = pd.get_dummies(X)\n","X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, stratify=y, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUgf2MFwYBy-","executionInfo":{"status":"ok","timestamp":1712565792199,"user_tz":-480,"elapsed":457191,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"d38dc7cb-8d9f-462d-e209-7a6754d14142"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Accuracy: 99.97%\n","Test Accuracy: 91.02%\n","CV Mean Score: 90.99%\n","CV Standard Deviation: 0.02%\n"]}],"source":["# Initialize the Random Forest classifier\n","model = RandomForestClassifier(n_estimators=100, bootstrap=True, random_state=42)\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","# Predictions for evaluation\n","y_train_pred = model.predict(X_train)\n","y_test_pred = model.predict(X_test)\n","\n","# Train and Test accuracy scores\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","\n","# Preprocessing for CV score\n","for col in X.columns:\n","    if X[col].dtype == 'object':\n","        X[col] = X[col].astype('category')\n","\n","# Perform 5-fold cross-validation to evaluate the model\n","cv_scores = cross_val_score(model, X_encoded, y, cv=5)\n","\n","# Calculate the mean and standard deviation of the cross-validation scores\n","cv_mean = cv_scores.mean()\n","cv_std = cv_scores.std()\n","print(f\"CV Mean Score: {cv_mean * 100:.2f}%\")\n","print(f\"CV Standard Deviation: {cv_std * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"xwKtAcaaYBy_"},"source":["#### 2.8.3 Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mf0ToU_YBy_"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Correctly create a preprocessing pipeline for numerical and categorical data\n","numeric_transformer = Pipeline(steps=[\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Add OneHotEncoder for categorical data\n","])\n","\n","# Identify numeric and categorical columns\n","numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n","categorical_features = X.select_dtypes(include=['object']).columns\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n","\n","# Apply preprocessing\n","X_preprocessed = preprocessor.fit_transform(X)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, stratify=y, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTa1_96_YBy_","executionInfo":{"status":"ok","timestamp":1712565828368,"user_tz":-480,"elapsed":34833,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"1c5d54e4-09f0-48c3-bae4-01516b4b3d4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Accuracy: 99.98%\n","Test Accuracy: 85.04%\n","CV Mean Score: 85.18%\n","CV Standard Deviation: 0.16%\n"]}],"source":["# Proceed with training and evaluating the Decision Tree model\n","model = DecisionTreeClassifier(random_state=42)\n","model.fit(X_train, y_train)\n","\n","y_train_pred = model.predict(X_train)\n","y_test_pred = model.predict(X_test)\n","\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","\n","# Preprocessing for CV score\n","for col in X.columns:\n","    if X[col].dtype == 'object':\n","        X[col] = X[col].astype('category')\n","\n","cv_scores = cross_val_score(model, X_preprocessed, y, cv=5)\n","cv_mean = cv_scores.mean()\n","cv_std = cv_scores.std()\n","print(f\"CV Mean Score: {cv_mean * 100:.2f}%\")\n","print(f\"CV Standard Deviation: {cv_std * 100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"lo8D-a0NYBy_"},"source":["### 2.10 Modelling (with parameters)\n","\n","We will only run the models on df2. The reason being - when we dropped values, there were 0 rows for df1 - hence there is no point running model on an empty dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwL4Ot-rYBzA"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import GridSearchCV\n","from imblearn.over_sampling import SMOTE\n","from imblearn.pipeline import Pipeline as ImbPipeline"]},{"cell_type":"markdown","metadata":{"id":"Rb9UwTBpYBzA"},"source":["#### Instantiating Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbYma9I4YBzA"},"outputs":[],"source":["# Logistic Regression Pipeline\n","imb_logistic_pipeline = ImbPipeline(steps=[\n","    ('scaler', StandardScaler()),\n","    ('smote', SMOTE(random_state=42)),  # Add SMOTE for oversampling\n","    ('model', LogisticRegression(max_iter=1000, random_state=42))\n","])\n","\n","# XGBoost Pipeline\n","imb_xgb_pipeline = ImbPipeline(steps=[\n","    ('smote', SMOTE(random_state=42)),\n","    ('model', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n","])\n","\n","# Random Forest Pipeline\n","imb_rf_pipeline = ImbPipeline(steps=[\n","    ('smote', SMOTE(random_state=42)),\n","    ('model', RandomForestClassifier(random_state=42))\n","])"]},{"cell_type":"markdown","metadata":{"id":"l7fTqSqKYBzA"},"source":["#### Parameter Grids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZajNgmkvYBzB"},"outputs":[],"source":["logistic_param_grid = {\n","    'model__C': [0.01, 0.1, 1, 10, 100],\n","    'model__penalty': ['l2'],\n","    'model__solver': ['saga'],\n","    'model__class_weight': [None, 'balanced'],  # Including class_weight\n","    'model__max_iter': [100, 1000, 5000]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kj8PyMtXYBzH"},"outputs":[],"source":["# Calculate class weights if your data is imbalanced\n","scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n","\n","# Define the parameter grid\n","xgb_param_grid = {\n","    'model__max_depth': [5, 7],\n","    'model__n_estimators': [100, 200],\n","    'model__learning_rate': [0.1],\n","    'model__scale_pos_weight': [1, scale_pos_weight],  # Use the ratio\n","    'model__subsample': [0.7, 1.0],\n","    'model__colsample_bytree': [0.7],\n","    'model__gamma': [0, 0.1]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRWEqT9rYBzH"},"outputs":[],"source":["rf_param_grid = {\n","    'model__n_estimators': [100, 200],\n","    'model__max_depth': [10, 20],\n","    'model__min_samples_split': [2, 5],\n","    'model__min_samples_leaf': [1, 2],\n","    'model__class_weight': [None, 'balanced', 'balanced_subsample'],\n","    'model__max_features': ['auto']\n","}"]},{"cell_type":"markdown","metadata":{"id":"bhJ7qA-lYBzH"},"source":["#### GridSearchCV Execution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k65Ao20KYBzH","executionInfo":{"status":"ok","timestamp":1712567237124,"user_tz":-480,"elapsed":1408770,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"b6b0a028-9cd2-4175-be6d-0cd24d581be6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters for Logistic Regression: {'model__C': 0.1, 'model__class_weight': None, 'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'saga'}\n","Best CV score for Logistic Regression: 0.7433510919429596\n","Train accuracy for Logistic Regression: 0.7434675422053577\n","Test accuracy for Logistic Regression: 0.7445213182918101\n","Mean CV score for the best Logistic Regression model: 0.7434 ± 0.0017\n"]}],"source":["%%time\n","\n","# Logistic Regression GridSearchCV\n","logistic_grid_search = GridSearchCV(imb_logistic_pipeline, logistic_param_grid, cv=5, scoring='accuracy')\n","logistic_grid_search.fit(X_train, y_train)\n","\n","# Best parameters and CV score\n","print(\"Best parameters for Logistic Regression:\", logistic_grid_search.best_params_)\n","print(\"Best CV score for Logistic Regression:\", logistic_grid_search.best_score_)\n","\n","# Evaluate on training data using the best estimator found by GridSearchCV\n","y_train_pred = logistic_grid_search.predict(X_train)\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","print(\"Train accuracy for Logistic Regression:\", train_accuracy)\n","\n","# Evaluate on test data using the best estimator found by GridSearchCV\n","y_test_pred = logistic_grid_search.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(\"Test accuracy for Logistic Regression:\", test_accuracy)\n","\n","# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n","# Here's how to get the mean CV score for the best estimator across folds\n","best_index1 = logistic_grid_search.best_index_\n","mean_cv_score = logistic_grid_search.cv_results_['mean_test_score'][best_index1]\n","std_cv_score = logistic_grid_search.cv_results_['std_test_score'][best_index1]\n","print(f\"Mean CV score for the best Logistic Regression model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSLNI8CQYBzJ"},"outputs":[],"source":["%%time\n","\n","# XGBoost GridSearchCV\n","xgb_grid_search = GridSearchCV(imb_xgb_pipeline, xgb_param_grid, cv=5, scoring='accuracy')\n","xgb_grid_search.fit(X_train, y_train)\n","\n","# Best parameters and score\n","print(\"Best parameters for XGBoost:\", xgb_grid_search.best_params_)\n","print(\"Best score for XGBoost:\", xgb_grid_search.best_score_)\n","\n","# Evaluate on training data using the best estimator found by GridSearchCV\n","y_train_pred = xgb_grid_search.predict(X_train)\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","print(\"Train accuracy for XGBoost:\", train_accuracy)\n","\n","# Evaluate on test data using the best estimator found by GridSearchCV\n","y_test_pred = xgb_grid_search.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(\"Test accuracy for XGBoost:\", test_accuracy)\n","\n","# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n","best_index = xgb_grid_search.best_index_\n","mean_cv_score = xgb_grid_search.cv_results_['mean_test_score'][best_index]\n","std_cv_score = xgb_grid_search.cv_results_['std_test_score'][best_index]\n","print(f\"Mean CV score for the best XGBoost model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8M9rwV1YBzJ"},"outputs":[],"source":["%%time\n","\n","# Random Forest GridSearchCV\n","rf_grid_search = GridSearchCV(imb_rf_pipeline, rf_param_grid, cv=5, scoring='accuracy')\n","rf_grid_search.fit(X_train, y_train)\n","\n","# Best parameters and score\n","print(\"Best parameters for Random Forest:\", rf_grid_search.best_params_)\n","print(\"Best score for Random Forest:\", rf_grid_search.best_score_)\n","\n","# Evaluate on training data using the best estimator found by GridSearchCV\n","y_train_pred = rf_grid_search.predict(X_train)\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","print(\"Train accuracy for Random Forest:\", train_accuracy)\n","\n","# Evaluate on test data using the best estimator found by GridSearchCV\n","y_test_pred = rf_grid_search.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(\"Test accuracy for Random Forest:\", test_accuracy)\n","\n","# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n","# Here's how to get the mean CV score for the best estimator across folds\n","best_index = rf_grid_search.best_index_\n","mean_cv_score = rf_grid_search.cv_results_['mean_test_score'][best_index]\n","std_cv_score = rf_grid_search.cv_results_['std_test_score'][best_index]\n","print(f\"Mean CV score for the best Random Forest model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n"]},{"cell_type":"markdown","source":["**Analysis:**\n","\n","Regarding the parameters hypertuned, please consider the following factors\n","\n","1) Parameter Grids:\n","\n","* The parameter grids for each model are relatively comprehensive but focused. This approach is a balance between exploring a range of parameter values and computational efficiency.\n","* For Logistic Regression and Random Forest, the parameters explored offer a good variety without being overly exhaustive. The choices for model__C, model__max_iter, and other parameters are within common ranges that typically yield good results.\n","* The SVM grid is more constrained, focusing on the RBF kernel and a narrower range of model__C and model__gamma values. This limitation is likely a practical decision to manage computational demand, as SVMs can become quite resource-intensive with larger parameter spaces and datasets.\n","* XGBoost's parameter grid is targeted, with a selection of parameters that are most influential on model performance (model__max_depth, model__n_estimators, etc.). The choices indicate an attempt to fine-tune the model around commonly effective values.\n","\n","2) Performance Metrics:\n","\n","* The performance metrics across models are very close, indicating that all four models perform similarly on this dataset under the chosen hyperparameters.\n","* The slight differences in CV scores, train accuracy, and test accuracy among the models could be due to the nature of the data and the models' inherent characteristics.\n","* The consistency in train and test accuracy suggests that the models are not overfitting significantly, which is positive.\n","\n","3) Hyperparameter Tuning:\n","\n","* The parameters do not appear to be \"over-tuned.\" The parameter ranges and values selected are reasonable and reflect common practice for balancing model complexity and performance.\n","* The models' good performance across both training and testing phases, with closely matched CV scores, suggests that the hyperparameter tuning has been effective in identifying robust configurations.\n","* One key aspect of hyperparameter tuning is ensuring that the model generalizes well to unseen data. The similarity between training and testing accuracy across models indicates successful hyperparameter tuning without overcomplicating the model.\n","\n","4) Model Selection:\n","* Given the similar performance of the models, the selection among them might come down to other factors such as interpretability, prediction speed, or specific use-case requirements.\n","* For instance, Logistic Regression offers good interpretability, SVMs can be effective for higher-dimensional data, XGBoost is known for its performance on structured/tabular data, and Random Forests are useful for their robustness and ease of use.\n","\n","5) Conclusion:\n","\n","* The hyperparameter tuning has been conducted thoughtfully to explore a meaningful range of configurations without overburdening computational resources.\n","* The models have achieved commendable accuracy, with slight variances that could guide model selection based on context-specific priorities.\n","* Further experiments might include exploring more nuanced hyperparameter spaces, incorporating feature selection or engineering, or applying different evaluation metrics relevant to the task at hand."],"metadata":{"id":"EM24osK20otv"}},{"cell_type":"markdown","source":["##### Determining the Confusion Matrix (using LogReg & SVM)\n","\n","Rationale: Since they have the closest Train and Test score"],"metadata":{"id":"0AFr2ZJr2IU3"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"OJFf3Y0Z2IDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict the target values based on the best estimator found by GridSearchCV\n","y_test_pred = logistic_grid_search.predict(X_test)\n","\n","# Compute the confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_test_pred)\n","\n","# Display the confusion matrix using Seaborn's heatmap\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n","plt.title('Confusion Matrix (Based on Logistic Regression Model)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.xticks(ticks=[0.5, 1.5], labels=['No Heart Disease', 'Heart Disease'])\n","plt.yticks(ticks=[0.5, 1.5], labels=['No Heart Disease', 'Heart Disease'], rotation=0)\n","plt.show()"],"metadata":{"id":"hBqI751L2Evu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict the target values based on the best estimator found by GridSearchCV\n","y_test_pred = svm_grid_search.predict(X_test)\n","\n","# Compute the confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_test_pred)\n","\n","# Display the confusion matrix using Seaborn's heatmap\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n","plt.title('Confusion Matrix (Based on SVM Model)')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.xticks(ticks=[0.5, 1.5], labels=['No Heart Disease', 'Heart Disease'])\n","plt.yticks(ticks=[0.5, 1.5], labels=['No Heart Disease', 'Heart Disease'], rotation=0)\n","plt.show()"],"metadata":{"id":"c_KVdZRF3UIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Creating Polynomial Features (Feature Engineering)"],"metadata":{"id":"jaKUHVC4H4uz"}},{"cell_type":"code","source":["#from sklearn.preprocessing import PolynomialFeatures\n","#from sklearn.feature_selection import SelectFromModel\n","\n","# Create interaction / polynomial features\n","#poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n","#X_poly = poly.fit_transform(X_train)"],"metadata":{"id":"X-g_eN6lDX6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Logistic Regression Pipeline with SelectFromModel\n","#logistic_pipeline = Pipeline(steps=[\n","    #('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n","    #('feature_selection', SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear'))),\n","    #('imputer', SimpleImputer(strategy='mean')),\n","    #('scaler', StandardScaler()),\n","    #('model', LogisticRegression(max_iter=1000, random_state=42))\n","#])\n","\n","# SVM Pipeline with SelectFromModel\n","#svm_pipeline = Pipeline([\n","    #('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n","    #('feature_selection', SelectFromModel(SVC(kernel='linear', C=0.01))),  # Linear kernel and a small C for feature selection\n","    #('imputer', SimpleImputer(strategy='mean')),\n","    #('scaler', StandardScaler()),\n","    #('model', SVC(kernel='rbf', random_state=42))  # Final SVM model\n","#])\n","\n","# XGBoost Pipeline with SelectFromModel\n","#xgb_pipeline = Pipeline([\n","    #('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n","    #('feature_selection', SelectFromModel(xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))),\n","    #('imputer', SimpleImputer(strategy='mean')),\n","    #('model', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))  # Final XGBoost model\n","#])\n","\n","\n","# Random Forest Pipeline with SelectFromModel\n","#rf_pipeline = Pipeline([\n","    #('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n","    #('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))),\n","    #('imputer', SimpleImputer(strategy='mean')),\n","    #('model', RandomForestClassifier(n_estimators=100, random_state=42))  # Final Random Forest model\n","#])"],"metadata":{"id":"HrBefAXUH27o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#logistic_param_grid = {\n","    #'feature_selection__estimator__C': [0.01, 0.1, 1, 10, 100], # hyperparameters for the logistic regression model used in SelectFromModel\n","    #'model__C': [0.01, 0.1, 1, 10, 100],\n","    #'model__penalty': ['l2'],\n","    #'model__solver': ['saga'],\n","    #'model__max_iter': [100, 1000, 5000]\n","#}\n","\n","#svm_param_grid = {\n","    #'model__C': [1, 10],  # Reduced range of C values\n","    #'model__gamma': ['scale', 0.01],  # Limited gamma to 'scale' and a representative value\n","    #'model__kernel': ['rbf'],  # Focus on the RBF kernel, often the best choice for SVM\n","    # Removed 'model__degree': Typically, 'poly' kernel requires more processing time\n","#}\n","\n","#xgb_param_grid = {\n","    #'model__max_depth': [5, 7],  # Focused on mid-range depths\n","    #'model__n_estimators': [100, 200],  # Reduced the upper range\n","    #'model__learning_rate': [0.1],  # Chose a commonly effective rate\n","    #'model__subsample': [0.7, 1.0],  # Limited to higher subsampling for variance reduction\n","    #'model__colsample_bytree': [0.7],  # Chose a moderate value for feature sampling\n","    #'model__gamma': [0, 0.1]  # Simplified to two options to evaluate regularization benefit\n","#}\n","\n","#rf_param_grid = {\n","    #'model__n_estimators': [100, 200],  # Reduced number of trees options\n","    #'model__max_depth': [10, 20],  # Focus on moderate to high depth to control complexity\n","    #'model__min_samples_split': [2, 5],  # Simplified range for minimum number of samples required to split\n","    #'model__min_samples_leaf': [1, 2],  # Reduced range for the minimum number of samples required at a leaf node\n","    #'model__max_features': ['auto'],  # Use the default option for the number of features to consider when looking for the best split\n","    #'model__bootstrap': [True]  # Keep bootstrapping enabled for better generalization\n","    # Removed 'model__criterion': Simplification, sticking with the default 'gini' criterion\n","}"],"metadata":{"id":"kFPiSjPHIVqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Logistic Regression GridSearchCV\n","#logistic_grid_search = GridSearchCV(logistic_pipeline, logistic_param_grid, cv=5, scoring='accuracy')\n","#logistic_grid_search.fit(X_train, y_train) # No need to transform X_train as pipeline will handle it\n","\n","# Best parameters and CV score\n","#print(\"Best parameters for Logistic Regression:\", logistic_grid_search.best_params_)\n","#print(\"Best CV score for Logistic Regression:\", logistic_grid_search.best_score_)\n","\n","# Evaluate on training data using the best estimator found by GridSearchCV\n","#y_train_pred = logistic_grid_search.predict(poly.transform(X_train))\n","#train_accuracy = accuracy_score(y_train, y_train_pred)\n","#print(\"Train accuracy for Logistic Regression:\", train_accuracy)\n","\n","#y_test_pred = logistic_grid_search.predict(poly.transform(X_test))\n","# Evaluate on test data using the best estimator found by GridSearchCV\n","#y_test_pred = logistic_grid_search.predict(poly.transform(X_test))\n","#test_accuracy = accuracy_score(y_test, y_test_pred)\n","#print(\"Test accuracy for Logistic Regression:\", test_accuracy)\n","\n","# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n","# Here's how to get the mean CV score for the best estimator across folds\n","#best_index1 = logistic_grid_search.best_index_\n","#mean_cv_score = logistic_grid_search.cv_results_['mean_test_score'][best_index1]\n","#std_cv_score = logistic_grid_search.cv_results_['std_test_score'][best_index1]\n","#print(f\"Mean CV score for the best Logistic Regression model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n"],"metadata":{"id":"-HjHAfF7Ibhk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# XGBoost GridSearchCV\n","#xgb_grid_search = GridSearchCV(xgb_pipeline, xgb_param_grid, cv=5, scoring='accuracy')\n","#xgb_grid_search.fit(X_train, y_train)\n","\n","# Best parameters and score\n","#print(\"Best parameters for XGBoost:\", xgb_grid_search.best_params_)\n","#print(\"Best score for XGBoost:\", xgb_grid_search.best_score_)\n","\n","# Evaluate on training data using the best estimator found by GridSearchCV\n","#y_train_pred = xgb_grid_search.predict(X_train)\n","#train_accuracy = accuracy_score(y_train, y_train_pred)\n","#print(\"Train accuracy for XGBoost:\", train_accuracy)\n","\n","# Evaluate on test data using the best estimator found by GridSearchCV\n","#y_test_pred = xgb_grid_search.predict(X_test)\n","#test_accuracy = accuracy_score(y_test, y_test_pred)\n","#print(\"Test accuracy for XGBoost:\", test_accuracy)\n","\n","# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n","# Here's how to get the mean CV score for the best estimator across folds\n","#best_index = xgb_grid_search.best_index_\n","#mean_cv_score = xgb_grid_search.cv_results_['mean_test_score'][best_index]\n","#std_cv_score = xgb_grid_search.cv_results_['std_test_score'][best_index]\n","#print(f\"Mean CV score for the best XGBoost model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n"],"metadata":{"id":"Apg91eckOZVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random Forest GridSearchCV\n","#rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='accuracy')\n","#rf_grid_search.fit(X_train, y_train)\n","\n","# Best parameters and score\n","#print(\"Best parameters for Random Forest:\", rf_grid_search.best_params_)\n","#print(\"Best score for Random Forest:\", rf_grid_search.best_score_)\n","\n","# Evaluate on training data using the best estimator found by GridSearchCV\n","#y_train_pred = rf_grid_search.predict(X_train)\n","#train_accuracy = accuracy_score(y_train, y_train_pred)\n","#print(\"Train accuracy for Random Forest:\", train_accuracy)\n","\n","# Evaluate on test data using the best estimator found by GridSearchCV\n","#y_test_pred = rf_grid_search.predict(X_test)\n","#test_accuracy = accuracy_score(y_test, y_test_pred)\n","#print(\"Test accuracy for Random Forest:\", test_accuracy)\n","\n","# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n","# Here's how to get the mean CV score for the best estimator across folds\n","#best_index = rf_grid_search.best_index_\n","#mean_cv_score = rf_grid_search.cv_results_['mean_test_score'][best_index]\n","#std_cv_score = rf_grid_search.cv_results_['std_test_score'][best_index]\n","#print(f\"Mean CV score for the best Random Forest model: {mean_cv_score:.4f} ± {std_cv_score:.4f}\")\n"],"metadata":{"id":"6FxwaidmIXMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion matrix values\n","TN = 4116\n","FP = 0\n","FN = 510\n","TP = 0\n","\n","# Calculate the total number of predictions\n","total_predictions = np.sum([TN, FP, FN, TP])\n","\n","# Calculate the percentages using numpy for consistency\n","TP_percentage = np.round((TP / total_predictions) * 100, 2)\n","TN_percentage = np.round((TN / total_predictions) * 100, 2)\n","FP_percentage = np.round((FP / total_predictions) * 100, 2)\n","FN_percentage = np.round((FN / total_predictions) * 100, 2)\n","\n","# Print the results\n","print(f\"True Positives Percentage: {TP_percentage}%\")\n","print(f\"True Negatives Percentage: {TN_percentage}%\")\n","print(f\"False Positives Percentage: {FP_percentage}%\")\n","print(f\"False Negatives Percentage: {FN_percentage}%\")"],"metadata":{"id":"LLrf-bt-1qF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n","\n","# Assuming svm_grid_search and X_test are already defined and svm_grid_search has been fitted\n","y_pred = logistic_grid_search.best_estimator_.predict(X_test)\n","\n","# Calculate precision, recall, F1 score, and ROC AUC score\n","precision = precision_score(y_test, y_pred, zero_division=0)\n","recall = recall_score(y_test, y_pred, zero_division=0)\n","f1 = f1_score(y_test, y_pred, zero_division=0)\n","roc_auc = roc_auc_score(y_test, y_pred)  # Ensure y_test and y_pred are appropriately prepared\n","\n","# Print the results\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n","print(f\"ROC AUC Score: {roc_auc:.4f}\")"],"metadata":{"id":"l7iAIOykjvXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict probabilities for the positive class\n","y_pred_prob = logistic_grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n","\n","# Calculate ROC AUC score using predicted probabilities\n","roc_auc = roc_auc_score(y_test, y_pred_prob)\n","print(f\"ROC AUC Score: {roc_auc:.4f}\")"],"metadata":{"id":"wGSrjrrejyes"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"colab":{"provenance":[],"machine_shape":"hm"}},"nbformat":4,"nbformat_minor":0}