{"cells":[{"cell_type":"markdown","metadata":{"id":"cpi2tMWgYBys"},"source":["<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n","\n","\n","# DSI-SG-42 Project 4:\n","###\n","---"]},{"cell_type":"markdown","metadata":{"id":"Y0xJqFM4YByx"},"source":["## 4. Modeling"]},{"cell_type":"markdown","metadata":{"id":"L3-mvMN_YByx"},"source":["### 4.1 Importing Libraries"]},{"cell_type":"code","execution_count":194,"metadata":{"id":"jHJQ_1UzYByy","executionInfo":{"status":"ok","timestamp":1712748200704,"user_tz":-480,"elapsed":415,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import xgboost as xgb\n","import time\n","\n","# For graphs and visualisations\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Essential library imports for Modeling Pre-Processing\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","\n","# Library to load the Models\n","from sklearn.linear_model import LogisticRegression\n","from xgboost import XGBClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# For Modeling Pipeline & Hyperparameter Tuning (GridSearchCV)\n","from imblearn.over_sampling import SMOTE\n","from imblearn.pipeline import Pipeline as ImbPipeline\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","\n","# For Modeling Metrics\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, f1_score\n","from sklearn.metrics import roc_curve, auc\n","\n","# For Deep Learning Neural Network\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, InputLayer\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau"]},{"cell_type":"markdown","metadata":{"id":"xrHxP_LmYBy0"},"source":["### 4.2 Import cleaned dataset"]},{"cell_type":"code","execution_count":195,"metadata":{"id":"vwebSFSrYBy1","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1712748203722,"user_tz":-480,"elapsed":1764,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"0439d7f1-0121-43d2-801e-87293079f418"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   height  weight     bmi  yrssmok  packday  sleep_hours   age  health_status  \\\n","0   1.744  81.376  28.054      0.0      0.0          8.0  80.0            2.0   \n","1   1.600  68.040  26.580      0.0      0.0          6.0  80.0            1.0   \n","2   1.570  63.500  25.760      0.0      0.0          5.0  56.0            2.0   \n","3   1.650  63.500  23.320     56.0      0.1          7.0  73.0            1.0   \n","4   1.570  53.980  21.900      0.0      0.0          9.0  43.0            4.0   \n","\n","   phys_health_not_good  mental_health_not_good  ...  asthma_status  \\\n","0                   1.0                     1.0  ...            3.0   \n","1                   1.0                     1.0  ...            3.0   \n","2                   2.0                     2.0  ...            3.0   \n","3                   1.0                     1.0  ...            1.0   \n","4                   2.0                     1.0  ...            3.0   \n","\n","   race_ethnicity  sex  education  income  smoker_status  e_cig_smoker  \\\n","0             1.0  2.0        4.0     7.0            4.0           1.0   \n","1             1.0  2.0        2.0     5.0            4.0           1.0   \n","2             1.0  2.0        4.0    10.0            4.0           1.0   \n","3             1.0  2.0        2.0     7.0            2.0           1.0   \n","4             1.0  2.0        3.0     5.0            4.0           1.0   \n","\n","   binge_drinker  heavy_drinker  chd_mi  \n","0            1.0            1.0     2.0  \n","1            1.0            1.0     2.0  \n","2            1.0            1.0     2.0  \n","3            1.0            1.0     2.0  \n","4            1.0            1.0     2.0  \n","\n","[5 rows x 28 columns]"],"text/html":["\n","  <div id=\"df-829d9322-d09f-4b2c-82ed-ebc1fa831bd7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>bmi</th>\n","      <th>yrssmok</th>\n","      <th>packday</th>\n","      <th>sleep_hours</th>\n","      <th>age</th>\n","      <th>health_status</th>\n","      <th>phys_health_not_good</th>\n","      <th>mental_health_not_good</th>\n","      <th>...</th>\n","      <th>asthma_status</th>\n","      <th>race_ethnicity</th>\n","      <th>sex</th>\n","      <th>education</th>\n","      <th>income</th>\n","      <th>smoker_status</th>\n","      <th>e_cig_smoker</th>\n","      <th>binge_drinker</th>\n","      <th>heavy_drinker</th>\n","      <th>chd_mi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.744</td>\n","      <td>81.376</td>\n","      <td>28.054</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>80.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>7.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.600</td>\n","      <td>68.040</td>\n","      <td>26.580</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>80.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.570</td>\n","      <td>63.500</td>\n","      <td>25.760</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>56.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>10.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.650</td>\n","      <td>63.500</td>\n","      <td>23.320</td>\n","      <td>56.0</td>\n","      <td>0.1</td>\n","      <td>7.0</td>\n","      <td>73.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.570</td>\n","      <td>53.980</td>\n","      <td>21.900</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>43.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 28 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-829d9322-d09f-4b2c-82ed-ebc1fa831bd7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-829d9322-d09f-4b2c-82ed-ebc1fa831bd7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-829d9322-d09f-4b2c-82ed-ebc1fa831bd7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-463bca83-1805-4c50-87dc-b6bba2819655\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-463bca83-1805-4c50-87dc-b6bba2819655')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-463bca83-1805-4c50-87dc-b6bba2819655 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":195}],"source":["df = pd.read_csv('sample_data/final_dataset.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"_sbP7ItQYBy2"},"source":["### 4.3 Checking the dataset"]},{"cell_type":"markdown","source":["#### 4.3.1 Checking columns and datatypes"],"metadata":{"id":"omtFmx-ztGd5"}},{"cell_type":"code","execution_count":196,"metadata":{"id":"Xpy8wg34YBy3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712748203722,"user_tz":-480,"elapsed":8,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"8c22bf74-c1bb-49e9-99e2-18c4a4594d7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 440111 entries, 0 to 440110\n","Data columns (total 28 columns):\n"," #   Column                     Non-Null Count   Dtype  \n","---  ------                     --------------   -----  \n"," 0   height                     440111 non-null  float64\n"," 1   weight                     440111 non-null  float64\n"," 2   bmi                        440111 non-null  float64\n"," 3   yrssmok                    440111 non-null  float64\n"," 4   packday                    440111 non-null  float64\n"," 5   sleep_hours                440111 non-null  float64\n"," 6   age                        440111 non-null  float64\n"," 7   health_status              440111 non-null  float64\n"," 8   phys_health_not_good       440111 non-null  float64\n"," 9   mental_health_not_good     440111 non-null  float64\n"," 10  last_routine_checkup       440111 non-null  float64\n"," 11  visit_dentist_past_year    440111 non-null  float64\n"," 12  health_insurance           440111 non-null  float64\n"," 13  phy_exercise_past_30_days  440111 non-null  float64\n"," 14  stroke                     440111 non-null  float64\n"," 15  cancer                     440111 non-null  float64\n"," 16  kidney_disease             440111 non-null  float64\n"," 17  colon_sigmoidoscopy        440111 non-null  float64\n"," 18  asthma_status              440111 non-null  float64\n"," 19  race_ethnicity             440111 non-null  float64\n"," 20  sex                        440111 non-null  float64\n"," 21  education                  440111 non-null  float64\n"," 22  income                     440111 non-null  float64\n"," 23  smoker_status              440111 non-null  float64\n"," 24  e_cig_smoker               440111 non-null  float64\n"," 25  binge_drinker              440111 non-null  float64\n"," 26  heavy_drinker              440111 non-null  float64\n"," 27  chd_mi                     440111 non-null  float64\n","dtypes: float64(28)\n","memory usage: 94.0 MB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"IGBo8LFHYBy4"},"source":["#### 4.3.2 Check for imbalanced dataset (using target variable - `chd_mi`)"]},{"cell_type":"code","source":["percentage_responses = df['chd_mi'].value_counts(normalize=True) * 100\n","\n","print(percentage_responses)"],"metadata":{"id":"REt3J1uaWqAp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712748203722,"user_tz":-480,"elapsed":5,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"87f12ab9-2849-4673-dc2b-f63bf899f270"},"execution_count":197,"outputs":[{"output_type":"stream","name":"stdout","text":["chd_mi\n","2.0    90.96796\n","1.0     9.03204\n","Name: proportion, dtype: float64\n"]}]},{"cell_type":"markdown","source":["**Analysis:**\n","\n","* We checked that there are no null values - all null values have been rectified at the data cleaning stage.\n","* There is a imbalance in the dataset - with 90.96% of respondents reported not having any heart disease or myocardial infarctions, while, while the remaining 9.03% reported having heart disease or myocardial infarctions. We will rectify the imbalance during the when fitting the model into the pipeline."],"metadata":{"id":"o0jCj1lh8xWE"}},{"cell_type":"markdown","metadata":{"id":"DfoeTKBbYBy5"},"source":["### 4.4 Modeling"]},{"cell_type":"markdown","source":["#### 4.4.1 Baseline Models"],"metadata":{"id":"oIWDoiHTt2gt"}},{"cell_type":"markdown","source":["Prior to modeling, the null values have been rectified in the Data Cleaning stage. For the purpose of this project, we will run look into 4 Classifier-type models as a baseline.\n","\n","- **Logistic Regression**\n","  - Simplicity and Interpretability: As a straightforward algorithm, it serves as a good starting point for binary classification tasks. It provides a probabilistic framework which means that besides making predictions, it can also quantify the uncertainty of its predictions, which is useful for understanding the impact of each feature on the prediction.\n","  - Performance: Despite its simplicity, Logistic Regression can perform quite well on linearly separable data.\n","  - Speed: It's computationally inexpensive, making it fast for both training and prediction, which is beneficial when working with very large datasets.\n","  - Baseline Comparison: Commonly used as a benchmark because of the ease to implement and interpret.\n","\n","- **XGBoost**\n","  - Accuracy: XGBoost is known for its high performance and speed in classification problems, using a gradient boosting framework.\n","  - Flexibility: XGBoost allows users to define custom optimization objectives and evaluation criteria, adding a layer of sophistication to the modeling.\n","\n","- **Random Forest**\n","  - Versatility: Random Forest performs well on a wide range of data types without the need for extensive data preprocessing like scaling and normalization.\n","  - Robustness: As an ensemble method, it is less prone to overfitting than a single decision tree and often has a very good performance right out of the box.\n","  - Feature Importance: It provides a straightforward indication of feature importance based on how much they contribute to reducing variance, which is helpful for feature selection.\n","\n","- **Decision Trees**\n","  \n","  - Non-Parametric: As a non-parametric method, it makes no assumptions about the underlying distributions of the data, which is useful for practical applications.\n","  - Handling Non-Linear Relationships: It can capture non-linear relationships between features and the target variable.\n","\n"],"metadata":{"id":"lx7HdhXyCqve"}},{"cell_type":"markdown","source":["##### 4.4.1.1 Logistic Regression"],"metadata":{"id":"Y_Td2ydgLpzk"}},{"cell_type":"code","source":["# Separate features and target from the cleaned dataframe\n","log_X = df.drop('chd_mi', axis=1)\n","log_y = df['chd_mi'].astype(int).map({1: 0, 2: 1})  # Map values and ensure int type\n","\n","# Encoding categorical variables\n","log_X_encoded = pd.get_dummies(log_X)\n","\n","# Ensuring log_X and log_y have consistent lengths\n","assert len(log_X_encoded) == len(log_y), \"log_X and log_y have inconsistent number of samples.\"\n","\n","# Perform the train-test split\n","log_X_train, log_X_test, log_y_train, log_y_test = train_test_split(log_X_encoded, log_y, test_size=0.2, stratify=log_y, random_state=42)"],"metadata":{"id":"vo6VbcQLL4B-","executionInfo":{"status":"ok","timestamp":1712748203723,"user_tz":-480,"elapsed":4,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}}},"execution_count":198,"outputs":[]},{"cell_type":"code","source":["# Feature scaling\n","scaler = StandardScaler()\n","log_X_train_scaled = scaler.fit_transform(log_X_train)\n","log_X_test_scaled = scaler.transform(log_X_test)"],"metadata":{"id":"oD1InGBlhiNv","executionInfo":{"status":"ok","timestamp":1712748204886,"user_tz":-480,"elapsed":6,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}}},"execution_count":199,"outputs":[]},{"cell_type":"code","source":["# Initialize the Logistic Regression model with a higher max_iter\n","model = LogisticRegression(max_iter=5000, random_state=42)\n","\n","# Train the model on the scaled data\n","model.fit(log_X_train_scaled, log_y_train)\n","\n","# Make predictions using the correctly scaled data\n","log_train_pred = model.predict(log_X_train_scaled)\n","log_test_pred = model.predict(log_X_test_scaled)\n","\n","# Calculate and print training and test accuracies\n","log_train_accuracy = accuracy_score(log_y_train, log_train_pred)\n","log_test_accuracy = accuracy_score(log_y_test, log_test_pred)\n","print(f\"LogReg Train Accuracy: {log_train_accuracy * 100:.2f}%\")\n","print(f\"LogReg Test Accuracy: {log_test_accuracy * 100:.2f}%\")\n","\n","# For cross-validation, ensure data is scaled before the process\n","log_cv_scores = cross_val_score(model, scaler.fit_transform(log_X_encoded), log_y, cv=5, scoring='accuracy')\n","\n","# Print the average of the cross-validation scores and scores for each fold\n","print(f\"LogReg Cross-Validation: {log_cv_scores.mean() * 100:.2f}%\")\n","print(f\"LogReg Cross-Validation (per Fold): {[f'{score * 100:.2f}%' for score in log_cv_scores]}\")"],"metadata":{"id":"tU0STPpfNNSK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712748213149,"user_tz":-480,"elapsed":8268,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"21d05090-77ab-43b7-a5b7-2d8bf3c0cf20"},"execution_count":200,"outputs":[{"output_type":"stream","name":"stdout","text":["LogReg Train Accuracy: 91.10%\n","LogReg Test Accuracy: 91.12%\n","LogReg Cross-Validation: 91.09%\n","LogReg Cross-Validation (per Fold): ['91.05%', '91.07%', '91.10%', '91.09%', '91.13%']\n"]}]},{"cell_type":"markdown","metadata":{"id":"jOay_pD4YBy7"},"source":["##### 4.4.1.2 XGBoost"]},{"cell_type":"code","execution_count":201,"metadata":{"id":"CST8I2BiYBy7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712748218679,"user_tz":-480,"elapsed":5546,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"bd284f68-c038-457a-a965-4cb68cc6b362"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n"]}],"source":["pip install xgboost"]},{"cell_type":"code","execution_count":202,"metadata":{"id":"_r6rrtLgYBy8","executionInfo":{"status":"ok","timestamp":1712748218680,"user_tz":-480,"elapsed":10,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}}},"outputs":[],"source":["# Separate features and target\n","xg_X = df.drop('chd_mi', axis=1)\n","xg_y = df['chd_mi'].astype(int)\n","\n","# Map the values of xg_y from [1, 2] to [0, 1]\n","xg_y_mapped = xg_y.map({1: 0, 2: 1})\n","\n","# Verify the consistency in the number of samples between xg_X and y_mapped\n","assert len(xg_X) == len(y_mapped), \"The feature set xg_X and target variable y_mapped have inconsistent lengths.\"\n","\n","# Now, you can safely perform the train-test split\n","xg_X_train, xg_X_test, xg_y_train, xg_y_test = train_test_split(xg_X, xg_y_mapped, test_size=0.2, stratify=xg_y_mapped, random_state=42)"]},{"cell_type":"code","execution_count":203,"metadata":{"id":"N2r2HPHOYBy9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712748246906,"user_tz":-480,"elapsed":28234,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"12aa0bcb-76fd-43ff-ba1a-3a450732384c"},"outputs":[{"output_type":"stream","name":"stdout","text":["XGBoost Train Accuracy: 91.65%\n","XGBoost Test Accuracy: 91.10%\n","XGBoost Cross-Validation: 91.06%\n","XGBoost Cross-Validation (per Fold): ['91.04%', '91.07%', '91.04%', '91.02%', '91.10%']\n"]}],"source":["# Initialize the XGBoost classifier with enable_categorical=True\n","model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', enable_categorical=True)\n","\n","# Train the model using the correct xg_y_train variable\n","model.fit(xg_X_train, xg_y_train)  # Use xg_y_train directly after ensuring it's correctly mapped and split\n","\n","# Make predictions on training and test sets\n","xg_train_pred = model.predict(xg_X_train)\n","xg_test_pred = model.predict(xg_X_test)\n","\n","# Calculate and print training and test accuracies\n","# Here, make sure to use 'xg_y_train' and 'xg_y_test' which are the variables you should have defined after the train-test split and mapping\n","xg_train_accuracy = accuracy_score(xg_y_train, xg_train_pred)\n","xg_test_accuracy = accuracy_score(xg_y_test, xg_test_pred)\n","print(f\"XGBoost Train Accuracy: {xg_train_accuracy * 100:.2f}%\")\n","print(f\"XGBoost Test Accuracy: {xg_test_accuracy * 100:.2f}%\")\n","\n","# Ensure X is suitable for cross-validation by converting object types to 'category' if needed\n","# This step might not be necessary for models like XGBoost when using enable_categorical=True\n","# but is kept for demonstration or if you plan to use models that do not natively support categorical features\n","xg_X_for_cv = xg_X.copy()\n","for col in xg_X_for_cv.columns:\n","    if xg_X_for_cv[col].dtype == 'object':\n","        xg_X_for_cv[col] = xg_X_for_cv[col].astype('category')\n","\n","# Perform 5-fold cross-validation using the mapped y\n","xg_cv_scores = cross_val_score(model, xg_X_for_cv, xg_y_mapped, cv=5, scoring='accuracy')\n","\n","# Print the average of the cross-validation scores and the scores for each fold\n","print(f\"XGBoost Cross-Validation: {xg_cv_scores.mean() * 100:.2f}%\")\n","print(f\"XGBoost Cross-Validation (per Fold): {[f'{score * 100:.2f}%' for score in xg_cv_scores]}\")"]},{"cell_type":"markdown","metadata":{"id":"B_wghPpPYBy-"},"source":["##### 4.4.1.3 Random Forest"]},{"cell_type":"code","execution_count":204,"metadata":{"id":"G-lh7jjDYBy-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712748252261,"user_tz":-480,"elapsed":5372,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}},"outputId":"23f1e945-be3a-478f-feaf-437ac0f7ad56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"]}],"source":["pip install scikit-learn"]},{"cell_type":"code","execution_count":205,"metadata":{"id":"TsKVgm7LYBy-","executionInfo":{"status":"ok","timestamp":1712748252261,"user_tz":-480,"elapsed":5,"user":{"displayName":"Amoz Kuang","userId":"03023043247296139234"}}},"outputs":[],"source":["# Separate features and target\n","rf_X = df.drop('chd_mi', axis=1)\n","rf_y = df['chd_mi']\n","\n","# Encoding categorical variables\n","rf_X_encoded = pd.get_dummies(rf_X)\n","\n","# Splitting dataset into training and testing sets\n","rf_X_train, rf_X_test, rf_y_train, rf_y_test = train_test_split(rf_X_encoded, rf_y, test_size=0.2, stratify=rf_y, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUgf2MFwYBy-"},"outputs":[],"source":["# Initialize the Random Forest classifier\n","model = RandomForestClassifier(n_estimators=100, bootstrap=True, random_state=42)\n","\n","# Training the model\n","model.fit(rf_X_train, rf_y_train)\n","\n","# Making predictions for evaluation\n","rf_y_train_pred = model.predict(rf_X_train)\n","rf_y_test_pred = model.predict(X_test)\n","\n","# Calculating Train and Test accuracy scores\n","rf_train_accuracy = accuracy_score(rf_y_train, rf_y_train_pred)\n","rf_test_accuracy = accuracy_score(rf_y_test, rf_y_test_pred)\n","print(f\"Random Forest Train Accuracy: {rf_train_accuracy * 100:.2f}%\")\n","print(f\"Random Forest Test Accuracy: {rf_test_accuracy * 100:.2f}%\")\n","\n","# Perform 5-fold cross-validation to evaluate the model\n","rf_cv_scores = cross_val_score(model, rf_X_encoded, rf_y, cv=5)\n","\n","# Calculate and print the average of the cross-validation scores\n","cv_mean = rf_cv_scores.mean()\n","cv_std = rf_cv_scores.std()\n","print(f\"Random Forest Cross-Validation: {rf_cv_scores.mean() * 100:.2f}%\")\n","print(f\"Random Forest Cross-Validation (per Fold): {[f'{score * 100:.2f}%' for score in rf_cv_scores]}\")"]},{"cell_type":"markdown","metadata":{"id":"xwKtAcaaYBy_"},"source":["##### 4.4.1.4 Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mf0ToU_YBy_"},"outputs":[],"source":["# Separate features and target\n","dt_X = df.drop('chd_mi', axis=1)\n","dt_y = df['chd_mi']\n","\n","# Separate numerical and categorical columns\n","numeric_features = dt_X.select_dtypes(include=['int64', 'float64']).columns\n","categorical_features = dt_X.select_dtypes(include=['object', 'category']).columns\n","\n","# Scale the numerical features\n","scaler = StandardScaler()\n","dt_X_numeric_scaled = scaler.fit_transform(dt_X[numeric_features])\n","\n","# One-hot encode the categorical features\n","encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","dt_X_categorical_encoded = encoder.fit_transform(dt_X[categorical_features])\n","\n","# Combine the processed numerical and categorical features back into a single dataset\n","dt_X_processed = np.concatenate([dt_X_numeric_scaled, dt_X_categorical_encoded], axis=1)\n","\n","# Now you have a fully processed dataset, you can split it into training and testing sets\n","dt_X_train, dt_X_test, dt_y_train, dt_y_test = train_test_split(dt_X_processed, dt_y, test_size=0.2, stratify=dt_y, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTa1_96_YBy_"},"outputs":[],"source":["# Initialize the Decision Tree model\n","model = DecisionTreeClassifier(random_state=42)\n","\n","# Train the model\n","model.fit(dt_X_train, dt_y_train)\n","\n","# Make predictions\n","dt_y_train_pred = model.predict(dt_X_train)\n","dt_y_test_pred = model.predict(dt_X_test)\n","\n","# Evaluate the model\n","dt_train_accuracy = accuracy_score(dt_y_train, dt_y_train_pred)\n","dt_test_accuracy = accuracy_score(dt_y_test, dt_y_test_pred)\n","print(f\"Decision Trees Train Accuracy: {dt_train_accuracy * 100:.2f}%\")\n","print(f\"Decision Trees Test Accuracy: {dt_test_accuracy * 100:.2f}%\")\n","\n","# Preprocessing for CV score\n","for col in dt_X.columns:\n","    if dt_X[col].dtype == 'object':\n","        dt_X[col] = dt_X[col].astype('category')\n","\n","dt_cv_scores = cross_val_score(model, dt_X_processed, dt_y, cv=5)\n","cv_mean = dt_cv_scores.mean()\n","cv_std = dt_cv_scores.std()\n","print(f\"Decision Trees Cross-Validation: {dt_cv_scores.mean() * 100:.2f}%\")\n","print(f\"Decision Trees Cross-Validation (per Fold): {[f'{score * 100:.2f}%' for score in dt_cv_scores]}\")"]},{"cell_type":"markdown","source":["**Analysis:**\n","\n","Albeit, the relatively high scores generally, the Logistic Regression and XGBoost models stood out in terms of:\n","\n","* Consistent Performance: The model has demonstrated high accuracy that is consistent across training, testing, and cross-validation. This consistency suggests that the model generalizes well and is not overfitting the data.\n","\n","* Simplicity and Interpretability: Logistic Regressionâ€™s simplicity is a considerable advantage. Itâ€™s faster to train and easier to interpret than more complex models. This is  beneficial when explaining the model's predictions and decisions to non-technical stakeholders.\n","\n","* Baseline Performance: Given that Logistic Regression is often used as a baseline model, its strong performance here validates its effectiveness for this particular dataset and problem. No doubt, XGBoost has a marginally higher training accuracy compared to Logistic Regression and holds its ground on test accuracy, suggesting it can capture complex patterns in the data without overfitting.\n","\n","* Robustness to Variance: Given the slightly better performance on the training set with no loss on the test set, XGBoost may be more robust to variance in the data than Logistic Regression.\n","\n","\n","To further the point, the reason why Random Forest and Decision Trees were not chosen are as follows:\n","\n","* Overfitting\n","  * The Random Forest training accuracy compared to its test accuracy indicates that it has overfit the training data.\n","  * The decision tree model shows an almost perfect training score but significantly lower test accuracy, which is a clear indicator of overfitting.\n","\n","* Computational Limitations: Random Forest can be more computationally expensive and time-consuming (8mins), which could be a concern for larger datasets or limited computational resources, especially when we include tuned parameters.\n","\n","* Poor Generalization:\n","  * The drop in cross-validation mean score in the Decision Trees model further supports the poor generalization capability of the decision tree model to unseen data compared to the other models.\n","  * Although there is high test accuracy in Random Forest, the discrepancy suggests that the model may not generalize as well to unseen data."],"metadata":{"id":"HoIkA4WPD1g9"}},{"cell_type":"markdown","metadata":{"id":"lo8D-a0NYBy_"},"source":["#### 4.4.2 Modeling with Parameters\n","\n","For the hyperparameter tuning, we will narrow in on Logistic Regression and XGBoost, and compare on our final model from there."]},{"cell_type":"markdown","metadata":{"id":"Rb9UwTBpYBzA"},"source":["##### 4.4.2.1 Initializing the Pipeline\n","\n","Initialize the pipelines for the LogReg and XGBoost model with the following items:\n","\n","* `Scaler`: We use `StandardScaler` here. Removes the mean and scales the graph to unit variance. This ensures consistency and efficiency of the train data when modeled.\n","* `SMOTE`: Synthetically Oversamples the Minority Class to balance the class distributed, using a small k-nearest neighbouring technique. Most suitable for imbalanced dataset such as the one we're using here. We initially considered the use of `ADASYN` to handle imbalance, however the time required to process was >4x times that required of the `SMOTE` technique. Moreoever, the results from the 2 techniques very marginally affects the Train-Test score of the hypertuned models (0.001).\n","* `Model`: Including the model as the last step in the pipeline allows for seamless integration of preprocessing steps and model training, facilitating a straightforward and reproducible process for model evaluation and selection.\n","  * Logistic Regression: The simplicity of the binary (0 or 1), and Probability (0 to 1) output often leads to ease of reader's interpretability.\n","  * XGBoost: Average target value for regression tasks or the log odds for a classification. The use of XGBoost will also help to regulate overfitting at an efficient and speedy manner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbYma9I4YBzA"},"outputs":[],"source":["# Logistic Regression Pipeline\n","imb_logistic_pipeline = ImbPipeline(steps=[\n","    ('scaler', StandardScaler()),\n","    ('smote', SMOTE(random_state=42, sampling_strategy=0.5)),\n","    ('model', LogisticRegression(random_state=42))\n","])\n","\n","# XGBoost Pipeline\n","imb_xgb_pipeline = ImbPipeline(steps=[\n","    ('scaler', StandardScaler()),\n","    ('smote', SMOTE(random_state=42, sampling_strategy=0.5)),\n","    ('model', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))\n","])"]},{"cell_type":"markdown","metadata":{"id":"l7fTqSqKYBzA"},"source":["##### 4.4.2.2 Parameter Grids\n","\n","The following parameters have been meticulously selected to fine-tune the models for optimal performance, especially when dealing with imbalanced and also huge dataset such as ours (400+K).\n","[Note: The following parameters below have a `model__` as a prefix]\n","\n","**1) Logistic Regression**\n","* `C`: Tests a range from strong to weak regularization to balance model complexity and accuracy.\n","* `penalty`: 'l2' helps manage multicollinearity and model generalization.\n","* `solver`: 'saga' is chosen for its efficiency with large data and support for 'l1' and 'l2' penalties.\n","* `class_weight`: 'balanced' adjusts weights inversely to class frequencies, addressing class imbalance.\n","* `max_iter`: Higher iterations ensure convergence for complex datasets.\n","\n","**2) XGBoost**\n","* `max_depth` and `n_estimators`: Control model complexity, balancing between capturing patterns and preventing overfitting.\n","* `learning_rate`: A moderate rate for effective learning without overshooting.\n","* `scale_pos_weight`: Adjusts positive to negative class weights for imbalanced data handling.\n","* `subsample` and `colsample_bytree`: Introduce randomness to prevent overfitting by selecting fractions of samples and features.\n","* `gamma`: Manages the trade-off between model simplicity and accuracy.\n","* `reg_lambda`: Further tuning to prevent overfitting, applying reg_lambda L2 regularization helps to smoothen the weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZajNgmkvYBzB"},"outputs":[],"source":["logistic_param_grid = {\n","    'model__C': [0.001, 0.01, 0.1, 1, 10],\n","    'model__penalty': ['l2'],\n","    'model__solver': ['saga'],\n","    'model__class_weight': ['balanced'],\n","    'model__max_iter': [500, 1000],\n","    'smote__sampling_strategy': [0.5, 0.75, 1.0],  # Trying different oversampling ratios\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kj8PyMtXYBzH"},"outputs":[],"source":["# Calculate class weights if your data is imbalanced\n","scale_pos_weight = sum(xg_y_train == 0) / sum(xg_y_train == 1)\n","\n","# Define the parameter grid\n","xgb_param_grid = {\n","    'model__max_depth': [5],\n","    'model__n_estimators': [100],\n","    'model__learning_rate': [0.1],\n","    'model__scale_pos_weight': [1, scale_pos_weight],  # Use both the ratio and 1\n","    'model__subsample': [0.7],\n","    'model__colsample_bytree': [0.7],\n","    'model__gamma': [0],\n","    'model__min_child_weight': [1, 5, 10],  # Important to prevent overfitting\n","    'model__reg_lambda': [0.5, 1, 1.5]  # Adding reg_lambda (L2 regularization term) for tuning\n","}"]},{"cell_type":"markdown","metadata":{"id":"bhJ7qA-lYBzH"},"source":["##### 4.4.2.3 Evaluating the Hypertuned Models (using GridSearchCV)"]},{"cell_type":"markdown","source":["**Metrics for consideration:**\n","Beyond just the CV and Train-test score in evaluating the hypertuned models, we should look into F1 score and ROC AUC score as well, since there is a class imbalance in our dataset.\n","\n","* F1 Score:\n","  - This is the harmonic mean of precision and recall, and it gives a balance between the two. In cases where there is a class imbalance, accuracy alone can be misleading; for instance, a model that predicts only the majority class will have high accuracy but poor recall. The F1 score becomes more informative because it will be low if either precision or recall is low. Thus, this captures the balance between the positive predictions and the positive actuals.\n","  - It's useful when you want to find a balance between the cost of false positives and false negatives.\n","\n","* ROC AUC Score:\n","  - Aggregate measure of performance across all classification thresholds. Unlike accuracy, the AUC ROC does not get influenced by the distribution of classes.\n","  - Evaluates the modelâ€™s ability to discriminate between the positive and negative classes. An AUC of 0.5 suggests no discrimination (i.e., random chance), while an AUC of 1.0 represents perfect discrimination\n","  - Predictions are ranked rather than their absolute values.\n"],"metadata":{"id":"hqiQC3n9FDR6"}},{"cell_type":"markdown","source":["###### 4.4.2.3.1 Logistic Regression"],"metadata":{"id":"nD1e2dWERqP3"}},{"cell_type":"code","source":["# Separate features and target from the cleaned dataframe\n","loghyp_X = df.drop('chd_mi', axis=1)\n","loghyp_y = df['chd_mi'].astype(int).map({1: 0, 2: 1})  # Map values and ensure int type\n","\n","# Encoding categorical variables\n","loghyp_X_encoded = pd.get_dummies(loghyp_X)\n","\n","# Ensuring loghyp_X and loghyp_y have consistent lengths\n","assert len(loghyp_X_encoded) == len(loghyp_y), \"loghyp_X and loghyp_y have inconsistent number of samples.\"\n","\n","# Perform the train-test split\n","loghyp_X_train, loghyp_X_test, loghyp_y_train, loghyp_y_test = train_test_split(loghyp_X_encoded, loghyp_y, test_size=0.2, stratify=loghyp_y, random_state=42)\n","\n","# Feature scaling\n","scaler = StandardScaler()\n","loghyp_X_train_scaled = scaler.fit_transform(loghyp_X_train)\n","loghyp_X_test_scaled = scaler.transform(loghyp_X_test)"],"metadata":{"id":"XSAK9nZKp4PH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k65Ao20KYBzH"},"outputs":[],"source":["%%time\n","# Use StratifiedKFold for handling imbalanced datasets\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# GridSearchCV with stratified cross-validation\n","logistic_grid_search = GridSearchCV(imb_logistic_pipeline, logistic_param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n","logistic_grid_search.fit(loghyp_X_train, loghyp_y_train)  # Using the training data for cross-validation\n","\n","# Best parameters and CV score\n","print(\"Best parameters for (hypertuned)Logistic Regression:\", logistic_grid_search.best_params_)\n","print(f\"Best CV score for (hypertuned) Logistic Regression: {logistic_grid_search.best_score_ * 100:.2f}%\")\n","\n","# Evaluate on training data using the best estimator found by GridSearchCV\n","loghyp_y_train_pred = logistic_grid_search.predict(loghyp_X_train)\n","loghyp_train_accuracy = accuracy_score(loghyp_y_train, loghyp_y_train_pred)\n","print(f\"Train accuracy for (hypertuned) Logistic Regression: {loghyp_train_accuracy * 100:.2f}%\")\n","\n","# Evaluate on test data using the best estimator found by GridSearchCV\n","loghyp_y_test_pred = logistic_grid_search.predict(loghyp_X_test)\n","loghyp_test_accuracy = accuracy_score(loghyp_y_test, loghyp_y_test_pred)\n","print(f\"Test accuracy for (hypertuned) Logistic Regression: {loghyp_test_accuracy * 100:.2f}%\")\n","\n","# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n","# Here's how to get the mean CV score for the best estimator across folds\n","loghyp_best_index = logistic_grid_search.best_index_\n","loghyp_mean_cv_score = logistic_grid_search.cv_results_['mean_test_score'][loghyp_best_index]\n","loghyp_std_cv_score = logistic_grid_search.cv_results_['std_test_score'][loghyp_best_index]\n","print(f\"Mean CV score for the best (hypertuned) Logistic Regression model: {loghyp_mean_cv_score * 100:.2f}% Â± {loghyp_std_cv_score * 100:.2f}%\")"]},{"cell_type":"code","source":["# Use the best estimator to make predictions on the test set\n","loghyp_best_estimator = logistic_grid_search.best_estimator_\n","loghyp_y_test_pred = loghyp_best_estimator.predict(loghyp_X_test_scaled)\n","loghyp_y_test_proba = loghyp_best_estimator.predict_proba(loghyp_X_test_scaled)[:, 1]  # Get probabilities for the positive class, ensure scaled data is used\n","\n","# Evaluation metrics\n","loghyp_conf_matrix = confusion_matrix(loghyp_y_test, loghyp_y_test_pred)\n","loghyp_class_report = classification_report(loghyp_y_test, loghyp_y_test_pred)\n","loghyp_roc_auc = roc_auc_score(loghyp_y_test, loghyp_y_test_proba)  # Use probabilities for ROC AUC score calculation\n","loghyp_f1 = f1_score(loghyp_y_test, loghyp_y_test_pred, average='binary')\n","\n","# Extracting loghyp_TP, loghyp_TN, loghyp_FP, loghyp_FN from the confusion matrix\n","loghyp_TP = loghyp_conf_matrix[1, 1]\n","loghyp_TN = loghyp_conf_matrix[0, 0]\n","loghyp_FP = loghyp_conf_matrix[0, 1]\n","loghyp_FN = loghyp_conf_matrix[1, 0]\n","\n","# Calculating Sensitivity, Specificity, Precision, and NPV\n","loghyp_sensitivity = loghyp_TP / (loghyp_TP + loghyp_FN)\n","loghyp_specificity = loghyp_TN / (loghyp_TN + loghyp_FP)\n","loghyp_precision = loghyp_TP / (loghyp_TP + loghyp_FP)\n","loghyp_npv = loghyp_TN / (loghyp_TN + loghyp_FN)\n","\n","# Print metrics\n","print(\"Confusion Matrix:\\n\", loghyp_conf_matrix)\n","print(\"\\nClassification Report:\\n\", loghyp_class_report)\n","print(f\"ROC AUC Score: {loghyp_roc_auc:.4f}\")\n","print(f\"F1 Score: {loghyp_f1:.4f}\")\n","print(f\"Sensitivity: {loghyp_sensitivity:.4f}\")\n","print(f\"Specificity: {loghyp_specificity:.4f}\")\n","print(f\"Precision: {loghyp_precision:.4f}\")\n","print(f\"NPV: {loghyp_:.4f}\")"],"metadata":{"id":"qzrevKoa2F8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remap loghyp_y_test values from {1.0, 2.0} to {0, 1}\n","loghyp_y_test_binary = np.where(loghyp_y_test == 2.0, 1, 0)\n","\n","# Now you can calculate FPR, TPR, and loghyp_thresholds for the ROC curve with the binary labels\n","loghyp_fpr, loghyp_tpr, loghyp_thresholds = roc_curve(loghyp_y_test_binary, loghyp_y_test_proba)\n","\n","# Calculate the AUC (Area Under Curve)\n","loghyp_roc_auc_value = auc(loghyp_fpr, loghyp_tpr)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(10, 8))\n","plt.plot(loghyp_fpr, loghyp_tpr, color='darkorange', lw=2, label=f'ROC curve (area = {loghyp_roc_auc_value:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC: LogReg Model')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"id":"BGU7FWP1Eur1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loghyp_conf_matrix = confusion_matrix(loghyp_y_test_binary, loghyp_y_test_pred, labels=[1, 0]).ravel()\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(loghyp_conf_matrix, annot=True, fmt='g', cmap='Blues',\n","            xticklabels=['Predicted Positive (1)', 'Predicted Negative (0)'],\n","            yticklabels=['Actual Positive (1)', 'Actual Negative (0)'])\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Coronary Heart Disease or Myocardial Infarction Occurrence: LogReg Model')\n","plt.show()"],"metadata":{"id":"nqeBpirxJHYD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Analysis:**\n","\n","* True Positives (TP): 65318 instances were correctly predicted as having the disease.\n","* True Negatives (TN): 4240 instances were correctly predicted as not having the disease.\n","* False Positives (FP): 3710 instances were incorrectly predicted as having the disease when they did not.\n","* False Negatives (FN): 14755 instances were incorrectly predicted as not having the disease when they actually did.\n","\n","**Implication on Metrics:**\n","\n","* Sensitivity/Recall for Positive Class (Heart Disease Present): High sensitivity [(TP)/(TP+FN)], indicating the model is good at catching positive cases.\n","* Specificity for Negative Class (Heart Disease Absent): Lower specificity [(TN)/(TN+FP)], indicating the model could be improved in correctly identifying those without the disease.\n","* Precision for Positive Class: [(TP)/(TP+FP)] is reasonably good, suggesting that when the model predicts the disease, it is often correct.\n","* Negative Predictive Value for Negative Class: [(TN)/(TN+FN)] indicates that there is room for improvement in predicting true negatives.\n","* High False Negatives: The model tends to miss a significant number of actual disease cases (high FN), which is a major concern in medical diagnosis because it means patients with the disease might not get the treatment they need.\n","* Accuracy: While not provided in the heatmap, it can be computed as [(TP+TN)/(TP+FP+FN+TN)]. The model appears to have a higher accuracy; however, due to the imbalance highlighted by the high FN, accuracy might not be the best standalone metric to rely upon.\n","* Precision-Recall Trade-off: It might be necessary to adjust the threshold for classification to improve either precision or recall, depending on which is more critical for the medical outcome. In medical diagnostics, improving recall (reducing FN) might be prioritized even if it comes at the cost of precision (increasing FP).\n","\n"],"metadata":{"id":"13vbRBB1_wo4"}},{"cell_type":"markdown","source":["###### 4.4.2.3.2 XGBoost"],"metadata":{"id":"PJa1R_coRz0w"}},{"cell_type":"code","source":["# Separate features and target\n","xghyp_X = df.drop('chd_mi', axghyp_xis=1)\n","xghyp_y = df['chd_mi'].astype(int)\n","\n","# Map the values of xghyp_y from [1, 2] to [0, 1]\n","xghyp_y_mapped = xghyp_y.map({1: 0, 2: 1})\n","\n","# Verify the consistency in the number of samples between xghyp_X and xghyp_y_mapped\n","assert len(xghyp_X) == len(xghyp_y_mapped), \"The feature set xghyp_X and target variable xghyp_y_mapped have inconsistent lengths.\"\n","\n","# Now, you can safely perform the train-test split\n","xghyp_X_train, xghyp_X_test, xghyp_y_train, xghyp_y_test = train_test_split(xghyp_X, xghyp_y_mapped, test_size=0.2, stratify=xghyp_y_mapped, random_state=42)"],"metadata":{"id":"VkNqTxdmMeRj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSLNI8CQYBzJ"},"outputs":[],"source":["%%time\n","\n","# XGBoost GridSearchCV\n","xgb_grid_search = GridSearchCV(imb_xgb_pipeline, xgb_param_grid, cv=5, scoring='accuracy')\n","xgb_grid_search.fit(xghyp_X_train, xghyp_y_train)\n","\n","# Best parameters and score\n","print(\"Best parameters for (hypertuned) XGBoost:\", xgb_grid_search.best_params_)\n","print(f\"Best score for (hypertuned) XGBoost: {xgb_grid_search.best_score_ * 100:.2f}%\")\n","\n","# Evaluate on training data using the best estimator found by GridSearchCV\n","xghyp_y_train_pred = xgb_grid_search.predict(xghyp_X_train)\n","xghyp_train_accuracy = accuracy_score(xghyp_y_train, xghyp_y_train_pred)\n","print(f\"Train accuracy for (hypertuned) XGBoost: {xghyp_train_accuracy * 100:.2f}%\")\n","\n","# Evaluate on test data using the best estimator found by GridSearchCV\n","xghyp_y_test_pred = xgb_grid_search.predict(xghyp_X_test)\n","xghyp_test_accuracy = accuracy_score(xghyp_y_test, xghyp_y_test_pred)\n","print(f\"Test accuracy for (hypertuned) XGBoost: {xghyp_test_accuracy * 100:.2f}%\")\n","\n","# If you want to report the CV scores detail for the best model, you can do it by accessing cv_results_\n","xghyp_best_index = xgb_grid_search.best_index_\n","xghyp_mean_cv_score = xgb_grid_search.cv_results_['mean_test_score'][xghyp_best_index]\n","xghyp_std_cv_score = xgb_grid_search.cv_results_['std_test_score'][xghyp_best_index]\n","print(f\"Mean CV score for the best (hypertuned) XGBoost model: {xghyp_mean_cv_score * 100:.2f}% Â± {xghyp_std_cv_score * 100:.2f}%\")"]},{"cell_type":"code","source":["# Feature scaling\n","scaler = StandardScaler()\n","xghyp_X_train_scaled = scaler.fit_transform(xghyp_X_train)\n","xghyp_X_test_scaled = scaler.transform(xghyp_X_test)\n","\n","# Use the best estimator to make predictions on the test set\n","xghyp_best_estimator = xgb_grid_search.best_estimator_\n","xghyp_y_test_pred = xghyp_best_estimator.predict(xghyp_X_test_scaled)  # Ensure xghyp_X_test is scaled\n","xghyp_y_test_proba = xghyp_best_estimator.predict_proba(xghyp_X_test)[:,1]  # Get probabilities for positive class\n","\n","# Evaluation metrics\n","xghyp_conf_matrix = confusion_matrix(xghyp_y_test, xghyp_y_test_pred)\n","xghyp_class_report = classification_report(xghyp_y_test, xghyp_y_test_pred)\n","xghyp_roc_auc = roc_auc_score(xghyp_y_test, xghyp_y_test_pred)\n","xghyp_f1 = f1_score(xghyp_y_test, xghyp_y_test_pred)\n","\n","# Extracting xghyp_TP, xghyp_TN, xghyp_FP, xghyp_FN from the confusion matrix\n","xghyp_TP = xghyp_conf_matrix[1, 1]\n","xghyp_TN = xghyp_conf_matrix[0, 0]\n","xghyp_FP = xghyp_conf_matrix[0, 1]\n","xghyp_FN = xghyp_conf_matrix[1, 0]\n","\n","# Calculating Sensitivity, Specificity, Precision, and NPV\n","xghyp_sensitivity = xghyp_TP / (xghyp_TP + xghyp_FN)\n","xghyp_specificity = xghyp_TN / (xghyp_TN + xghyp_FP)\n","xghyp_precision = xghyp_TP / (xghyp_TP + xghyp_FP)\n","xghyp_npv = xghyp_TN / (xghyp_TN + xghyp_FN)\n","\n","# Print metrics\n","print(\"Confusion Matrix:\\n\", xghyp_conf_matrix)\n","print(\"\\nClassification Report:\\n\", xghyp_class_report)\n","print(\"ROC AUC Score:\", xghyp_roc_auc)\n","print(\"F1 Score:\", xghyp_f1)\n","print(f\"Sensitivity: {xghyp_sensitivity:.4f}\")\n","print(f\"Specificity: {xghyp_specificity:.4f}\")\n","print(f\"Precision: {xghyp_precision:.4f}\")\n","print(f\"NPV: {xghyp_npv:.4f}\")"],"metadata":{"id":"DWQ-Opd8NMNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xghyp_y_test_pred = xghyp_best_estimator.predict(xghyp_X_test_scaled)  # Predictions are made here\n","xghyp_y_test_proba = xghyp_best_estimator.predict_proba(xghyp_X_test_scaled)[:, 1]  # Probabilities for the positive class\n","\n","# Calculate FPR, TPR, and threshold values\n","xghyp_fpr, xghyp_tpr, xghyp_thresholds = roc_curve(xghyp_y_test, xghyp_y_test_proba)\n","\n","# Calculate the AUC (Area Under the Curve)\n","xghyp_roc_auc = auc(xghyp_fpr, xghyp_tpr)\n","\n","# Plotting\n","plt.figure(figsize=(8, 6))\n","plt.plot(xghyp_fpr, xghyp_tpr, color='darkorange', lw=2, label=f'ROC curve (area = {xghyp_roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal 45 degree line\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve: XGBoost Model')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"id":"DFqg-FyIralE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming '1' in your target variable y represents the presence of the disease\n","xghyp_conf_matrix = confusion_matrix(xghyp_y_test, xghyp_y_test_pred, labels=[1, 0]).ravel()  # Ensure labels are ordered as [1, 0] for TP, FN, FP, TN\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(xghyp_conf_matrix, annot=True, fmt='g', cmap='Blues',\n","            xticklabels=['Predicted Positive (1)', 'Predicted Negative (0)'],\n","            yticklabels=['Actual Positive (1)', 'Actual Negative (0)'])\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Coronary Heart Disease or Myocardial Infarction Occurrence: XGBoost Model')\n","plt.show()"],"metadata":{"id":"f3NP8LWvr2xD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Analysis:**\n","* True Positives (TP): 22388 instances where the model correctly predicted the presence of the disease.\n","* True Negatives (TN): 5849 instances where the model correctly predicted the absence of the disease.\n","* False Positives (FP): 2101 instances where the model incorrectly predicted the presence of the disease.\n","* False Negatives (FN): 57685 instances where the model incorrectly predicted the absence of the disease.\n","\n","**Implication on Metrics:**\n","* The model seems to have a low sensitivity/recall as it misses a lot of true cases (high FN).\n","* The model's specificity is better in this case, indicating it's relatively more accurate when predicting the absence of the disease.\n","* Precision (the proportion of positive identifications that were actually correct) seems moderate but is affected by a relatively high number of false positives.\n","* The high number of false negatives is particularly concerning, suggesting that many individuals with the disease may not be identified by the model, which could have serious implications in a healthcare setting."],"metadata":{"id":"Ob-EfNP8Dp0n"}},{"cell_type":"markdown","source":["**Conclusion on both (hypertuned) LogReg and XGBoost Models:**\n","\n"," It's normal and expected for scores to change - in this case, drop - as the model's focus shifts due to parameter tuning. The goal of tuning is often to improve *generalization*, not necessarily to increase the raw train-test accuracy score.\n","\n","Hence, performance on specific aspects (like the ROC AUC score should be considered. The ROC AUC score is particularly useful in evaluating models on imbalanced datasets -such as in our case - because it considers both the positive and negative classes through the entire range of classification thresholds.\n","\n","* ROC score LogReg:\n","* ROC score XGBoost:"],"metadata":{"id":"2somvZq8_j3P"}},{"cell_type":"markdown","source":["---\n","#### 4.4.3 Deep Learning Modeling (with Keras)\n","\n","Beyond the traditional types of modeling, we will also explore a Feedforward Neural Network (FNN), specifically designed for binary classification tasks, to compare with our hypertuned \"traditional\" models. This type of network is also commonly referred to as a Multilayer Perceptron (MLP) when it includes one or more hidden layers, as is the case here.\n","\n","In the case here, where we're determining the predictability of a binary classification task,(having heart disease/myocardial infarction or not), we will adopt `binary crossentropy` as the loss function. This loss function is designed for two-class classification problems.\n","\n","As for the activation function, the `sigmoid` is appropriate for binary classification tasks when using neural networks. It will output a value between 0 and 1, which is typically interpreted as the probability of belonging to the positive class.\n","\n","* Performance Metrics: Train-Test Accuracy Score is will be used to compare against the traditional models, to evaluate the model's performance, indicating how often the model correctly classifies the input data."],"metadata":{"id":"0AFr2ZJr2IU3"}},{"cell_type":"markdown","source":["##### 4.4.3.1 Binary Crossentropy"],"metadata":{"id":"y5AN4hgPzHi-"}},{"cell_type":"code","source":["pip install tensorflow keras scikit-learn"],"metadata":{"id":"nvhCfLp5aRTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separate features and target from the cleaned dataframe\n","fnn_X = df.drop('chd_mi', axis=1)\n","fnn_y = df['chd_mi'].astype(int).map({1: 0, 2: 1})  # Map values and ensure int type\n","\n","# # Encoding categorical variables\n","# loghyp_X_encoded = pd.get_dummies(loghyp_X)\n","\n","# # Ensuring loghyp_X and loghyp_y have consistent lengths\n","# assert len(loghyp_X_encoded) == len(loghyp_y), \"loghyp_X and loghyp_y have inconsistent number of samples.\"\n","\n","# Perform the train-test split\n","fnn_X_train, fnn_X_test, fnn_y_train, fnn_y_test = train_test_split(fnn_X, fnn_y, test_size=0.2, stratify=fnn_y, random_state=42)\n","\n","# Feature scaling\n","scaler = StandardScaler()\n","fnn_X_train_scaled = scaler.fit_transform(fnn_X_train)\n","fnn_X_test_scaled = scaler.transform(fnn_X_test)"],"metadata":{"id":"CImH9n7Dt9pN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the Keras model with added dropout for regularization\n","model = Sequential()\n","model.add(Dense(64, input_dim=fnn_X_train_scaled.shape[1], activation='relu'))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n","\n","# Compile the model with a potentially more effective optimizer\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Early stopping and learning rate reduction on plateau\n","callbacks = [\n","    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),\n","    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n","]\n","\n","# Fit the model with a larger batch size to speed up training\n","history = model.fit(\n","    fnn_X_train_scaled, fnn_y_train,\n","    validation_split=0.2,\n","    epochs=50,\n","    batch_size=128,  # Larger batch size to speed up training\n","    verbose=1,\n","    callbacks=callbacks  # Use callbacks for early stopping and LR reduction\n",")\n","\n","# Predict probabilities for the test set (ROC AUC Score)\n","fnn_y_test_proba = model.predict(fnn_X_test_scaled, verbose=0)\n","\n","# Evaluate the model (Train-Test Score, ROC AUC Score)\n","fnn_train_acc = model.evaluate(fnn_X_train_scaled, fnn_y_train, verbose=0)\n","fnn_test_acc = model.evaluate(fnn_X_test_scaled, fnn_y_test, verbose=0)\n","fnn_roc_auc = roc_auc_score(fnn_y_test, fnn_y_test_proba)\n","fnn_f1 = f1_score(fnn_y_test, fnn_y_test_proba)\n","print(f'FNN Train Accuracy: {fnn_train_acc * 100:.2f}%')\n","print(f'FNN Test Accuracy: {fnn_test_acc * 100:.2f}%')\n","print(f'FNN ROC AUC Score: {fnn_roc_auc:.4f}')\n","print(f'FNN F1 Score: {fnn_f1:.4f}')"],"metadata":{"id":"1E3RYZECGAOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"YdK8YXoIxTWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot accuracy or loss over epochs\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='validation')\n","plt.title('FNN Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train Accuracy', 'Validation Accuracy'], loc='upper left')\n","plt.show()"],"metadata":{"id":"AGwFWd5Pw0ub"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot training & validation loss values\n","plt.figure(figsize=(10, 5))\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('FNN Model Loss', size=17)\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper right')\n","plt.show()"],"metadata":{"id":"NFhXfnYqw2nP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict probabilities for the test set (ROC AUC Score)\n","fnn_y_test_proba = model.predict(fnn_X_test_scaled, verbose=0)\n","\n","# Convert probabilities to binary predictions\n","# You need to choose a threshold for classification, usually 0.5 for binary classification\n","threshold = 0.5\n","fnn_y_test_pred = np.where(fnn_y_test_proba.flatten() > threshold, 1, 0)\n","\n","# Compute the confusion matrix using the true labels (y_test) and predicted labels (y_test_pred)\n","fnn_conf_matrix = confusion_matrix(fnn_y_test, fnn_y_test_pred)\n","\n","# Plotting the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(fnn_conf_matrix, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=['Predicted Negative (0)', 'Predicted Positive (1)'],\n","            yticklabels=['Actual Negative (0)', 'Actual Positive (1)'])\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Confusion Matrix: FNN Model')\n","plt.show()"],"metadata":{"id":"zRHWHOFwbmDl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["asda"],"metadata":{"id":"MoYevaJ1-dbF"}},{"cell_type":"markdown","source":["---\n","\n","### 4.5 Conclusion\n","\n","---"],"metadata":{"id":"XzR8BTXhkkqK"}},{"cell_type":"markdown","source":["#### 4.5.1 Summary\n"],"metadata":{"id":"WnTipj9ymZj4"}},{"cell_type":"markdown","source":["#### 4.5.2 Limitations\n","\n","The models w\n","\n","Limitations of Logistic Regression:\n","\n","Assumption of Linearity: Logistic Regression assumes a linear relationship between the independent variables and the log odds of the dependent variable. This can limit its ability to model more complex, non-linear relationships that might exist in the data.\n","Feature Importance: It may not perform well with a large number of categorical features or variables that are interrelated (multicollinearity). Feature scaling and selection become critical.\n","Binary Outcomes: It is primarily used for binary classification problems. Although it can be extended to multiclass classification, it may not perform as well as inherently multiclass models.\n","\n","Limitations of XGBoost:\n","\n","Overfitting: If not carefully tuned, XGBoost can overfit to the training data, especially when the data is noisy or when there are too many trees.\n","Computationally Intensive: Hyperparameter tuning for XGBoost can be computationally expensive, as it involves training multiple gradient boosting trees with cross-validation.\n","Interpretability: While it does offer some level of interpretability through feature importance scores, understanding how the model arrived at a prediction can be more challenging compared to simpler models like Logistic Regression."],"metadata":{"id":"_QOEJd0GnCNq"}},{"cell_type":"markdown","source":["#### 4.5.3 Recommendations"],"metadata":{"id":"LQW9JUTPnOiK"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"colab":{"provenance":[],"machine_shape":"hm"}},"nbformat":4,"nbformat_minor":0}